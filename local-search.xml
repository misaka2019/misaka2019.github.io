<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Text-CNN</title>
    <link href="/2021/01/29/Text-CNN/"/>
    <url>/2021/01/29/Text-CNN/</url>
    
    <content type="html"><![CDATA[<h3 id="Text-CNN"><a href="#Text-CNN" class="headerlink" title="Text-CNN"></a>Text-CNN</h3><h4 id="一维卷积"><a href="#一维卷积" class="headerlink" title="一维卷积"></a>一维卷积</h4><p>一维卷积通常有三种类型：full卷积、same卷积和valid卷积，下面以一个长度为5的一维张量I和长度为3的一维张量K（卷积核）为例，介绍这三种卷积的计算过程</p><h5 id="一维Full卷积"><a href="#一维Full卷积" class="headerlink" title="一维Full卷积"></a>一维Full卷积</h5><p>Full卷积的计算过程是：K沿着I顺序移动，每移动到一个固定位置，对应位置的值相乘再求和，计算过程如下：</p><p><img src="https://img2018.cnblogs.com/blog/1244340/201907/1244340-20190712174053004-1773413676.png" alt="img"></p><p>将得到的值依次存入一维张量Cfull，该张量就是I和卷积核K的full卷积结果，其中K卷积核或者滤波器或者卷积掩码，卷积符号用符号★表示，记Cfull=I★K</p><p><img src="https://img2018.cnblogs.com/blog/1244340/201907/1244340-20190712172621556-503131905.png" alt="img"></p><h5 id="一维Same卷积"><a href="#一维Same卷积" class="headerlink" title="一维Same卷积"></a>一维Same卷积</h5><p>卷积核K都有一个锚点，然后将锚点顺序移动到张量I的每一个位置处，对应位置相乘再求和，计算过程如下：</p><p><img src="https://img2018.cnblogs.com/blog/1244340/201907/1244340-20190712174211681-1549927797.png" alt="img"></p><p>假设卷积核的长度为FL，如果FL为奇数，锚点位置在(FL-1)/2处；如果FL为偶数，锚点位置在(FL-2)/2处。</p><h5 id="一维Valid卷积"><a href="#一维Valid卷积" class="headerlink" title="一维Valid卷积"></a><strong>一维Valid卷积</strong></h5><p>从full卷积的计算过程可知，如果K靠近I，就会有部分延伸到I之外，valid卷积只考虑I能完全覆盖K的情况，即K在I的内部移动的情况，计算过程如下：</p><p><img src="https://img2018.cnblogs.com/blog/1244340/201907/1244340-20190712174137660-2128836425.png" alt="img"></p><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p><img src="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/11/Screen-Shot-2015-11-06-at-8.03.47-AM.png" alt="网络结构"></p><p>TextCNN是利用卷积神经网络来处理文本分类的一种模型。其思想与计算机视觉中的CNN大同小异。</p><h4 id="参数与超参数"><a href="#参数与超参数" class="headerlink" title="参数与超参数"></a>参数与超参数</h4><ul><li>sequence_length<br>Q: 对于CNN, 输入与输出都是固定的，可每个句子长短不一, 怎么处理?<br>A: 需要做定长处理, 比如定为n, 超过的截断, 不足的补0. 注意补充的0对后面的结果没有影响，因为后面的max-pooling只会输出最大值，补零的项会被过滤掉.</li><li>num_classes<br>多分类, 分为几类.</li><li>vocabulary_size<br>语料库的词典大小, 记为|D|.</li><li>embedding_size<br>将词向量的维度, 由原始的 |D| 降维到 embedding_size.</li><li>filter_size_arr<br>多个不同size的filter.</li></ul><h4 id="Embedding-Layer"><a href="#Embedding-Layer" class="headerlink" title="Embedding Layer"></a>Embedding Layer</h4><p>嵌入层是通过一个隐藏层, 将 one-hot 编码的词 投影 到一个低维空间中。<br>本质上是特征提取器，在指定维度中编码语义特征. 这样, 语义相近的词, 它们的欧氏距离或余弦距离也比较近。</p><h4 id="Convolution-Layer"><a href="#Convolution-Layer" class="headerlink" title="Convolution Layer"></a>Convolution Layer</h4><p>为不同尺寸的 filter 都建立一个卷积层. 所以会有多个 feature map.<br>图像是像素点组成的二维数据, 有时还会有RGB三个通道, 所以它们的卷积核至少是二维的.<br>从某种程度上讲, word is to text as pixel is to image, 所以这个卷积核的 size 与 stride 会有些不一样.</p><ul><li>xixi<br>xi∈Rkxi∈Rk, 一个长度为n的句子中, 第 i 个词语的词向量, 维度为k.</li><li>xi:jxi:j<br>xi:j=xi⊕xi+1⊕…⊕xjxi:j=xi⊕xi+1⊕…⊕xj<br>表示在长度为n的句子中, 第 [i,j] 个词语的词向量的拼接.</li><li>hh<br>卷积核所围窗口中单词的个数, 卷积核的尺寸其实就是 hkhk.</li><li>ww<br>w∈Rhkw∈Rhk, 卷积核的权重矩阵.</li><li>cici<br>ci=f(w⋅xi:i+h−1+b)ci=f(w⋅xi:i+h−1+b), 卷积核在单词i位置上的输出. b∈RKb∈RK, 是 bias. ff 是双曲正切之类的激活函数.</li><li>c=[c1,c2,…,cn−h+1]c=[c1,c2,…,cn−h+1]<br>filter在句中单词上进行所有可能的滑动, 得到的 feature mapfeature map.</li></ul><h4 id="Max-Pooling-Layer"><a href="#Max-Pooling-Layer" class="headerlink" title="Max-Pooling Layer"></a>Max-Pooling Layer</h4><p>max-pooling只会输出最大值, 对输入中的补0 做过滤.</p><h4 id="Fulled-Connected-Layer"><a href="#Fulled-Connected-Layer" class="headerlink" title="Fulled-Connected Layer"></a>Fulled-Connected Layer</h4><p>使用dropout和softmax函数输出每个类别的概率。</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>textcnn</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RNN、LSTM及其变体</title>
    <link href="/2021/01/23/rnn/"/>
    <url>/2021/01/23/rnn/</url>
    
    <content type="html"><![CDATA[<h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>RNN是一类用于处理序列数据的神经网络。序列数据就是后面的数据和前面的数据有关系的数据。</p><h4 id="RNN的结构及变体"><a href="#RNN的结构及变体" class="headerlink" title="RNN的结构及变体"></a>RNN的结构及变体</h4><p>神经网络包含输入层、隐藏层、输出层，通过激活函数控制输出，层与层之间通过权值连接。激活函数是事先确定好的，那么神经网络模型通过训练“学”到的东西就蕴含在“权值”中。</p><p>基础的神经网络只在层与层之间建立了权连接，RNN最大的不同之处就是在层之间的神经元之间也建立的权连接。如图。</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZXMyMDE1LmNuYmxvZ3MuY29tL2Jsb2cvMTA0MjQwNi8yMDE3MDMvMTA0MjQwNi0yMDE3MDMwNjE0MjI1MzM3NS0xNzU5NzE3NzkucG5n?x-oss-process=image/format,png" alt="img"></p><p>这是一个标准的RNN结构图，图中每个箭头代表做一次变换，也就是说箭头连接带有权值。左侧是折叠起来的样子，右侧是展开的样子，左侧旁边的箭头代表此结构中的“循环”体现在隐藏层。</p><p>在展开结构中我们可以观察到，在标准的RNN结构中，隐层的神经元之间也是带有权值的。也就是说，随着序列的不断推进，前面的隐层将会影响后面的隐层。图中O代表输出，y代表样本给出的确定值，L代表损失函数，我们可以看到，“损失“也是随着序列的推荐而不断积累的。</p><p>除上述特点以为，标准的RNN还有以下特点：</p><ol><li>权值共享，共用三组权值，即W、U、V。</li><li>每一个输入值都只与它本身的那条路线建立权连接，不会和别的神经元连接。</li></ol><p>以上是RNN的标准结构，然而在实际中这一种结构并不能解决所有的问题，例如我们做文本分类，输入为一串文字，输出为类别，那么就只需要单个输出。如图。</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL3AwLmlmZW5naW1nLmNvbS9wbW9wLzIwMTcvMDkwMS8zOTRBNDZFRTE1MjcwMkZGM0Q1NjQxMTQ3OTE3QTMxN0ExRTZCNzMyX3NpemUyOV93NjAwX2g0NjAuanBlZw?x-oss-process=image/format,png" alt="这里写图片描述"></p><p>同样的，我们有时候还需要单输入但是输出为序列的情况。那么就可以使用如下结构：</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL3AwLmlmZW5naW1nLmNvbS9wbW9wLzIwMTcvMDkwMS83MEJBOTM4OUU2M0VDMzE2QUIwQzg5QzRFRDE1RkFBMUM1RTVEQjFDX3NpemUyM193NjAwX2g0NzAuanBlZw?x-oss-process=image/format,png" alt="img"></p><p>还有一种结构是输入虽是序列，但不随着序列变化，就可以使用如下结构：</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL3AwLmlmZW5naW1nLmNvbS9wbW9wLzIwMTcvMDkwMS81MUYyM0Q0NTIxMEVGN0Y3MzBBOTI3RENGMzJGMkJGRjBEQkI3QTY1X3NpemUzMF93NjAwX2g0NjguanBlZw?x-oss-process=image/format,png" alt="这里写图片描述"></p><p>原始的N vs N RNN要求序列等长，然而我们遇到的大部分问题序列都是不等长的，如机器翻译中，源语言和目标语言的句子往往并没有相同的长度。<br>下面我们来介绍RNN最重要的一个变种：N vs M。这种结构又叫Encoder-Decoder模型，也可以称之为Seq2Seq模型。</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL3AwLmlmZW5naW1nLmNvbS9wbW9wLzIwMTcvMDkwMS83MjYxNUFDODNEMTFFQkQ2RTczMjA4RkE4Q0UwOTMyRDNEMEU1RkU5X3NpemUyNV93NjAwX2gzOTEuanBlZw?x-oss-process=image/format,png" alt="这里写图片描述"></p><p>从名字就能看出，这个结构的原理是先编码后解码。左侧的RNN用来编码得到c，拿到c后再用右侧的RNN进行解码。得到c有多种方式，最简单的方法就是把Encoder的最后一个隐状态赋值给c，还可以对最后的隐状态做一个变换得到c，也可以对所有的隐状态做变换。</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL3AwLmlmZW5naW1nLmNvbS9wbW9wLzIwMTcvMDkwMS9DQjg4NkZGRDU1RDQ4QjRGNkUxODNENDc5NEUxRjMwNUZGQzI1RkM5X3NpemUxN193NjAwX2gyNDEuanBlZw?x-oss-process=image/format,png" alt="这里写图片描述"></p><h4 id="标准RNN的前向输出流程"><a href="#标准RNN的前向输出流程" class="headerlink" title="标准RNN的前向输出流程"></a>标准RNN的前向输出流程</h4><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZXMyMDE1LmNuYmxvZ3MuY29tL2Jsb2cvMTA0MjQwNi8yMDE3MDMvMTA0MjQwNi0yMDE3MDMwNjE0MjI1MzM3NS0xNzU5NzE3NzkucG5n?x-oss-process=image/format,png" alt="这里写图片描述"></p><p>各个符号的含义：x是输入，h是隐层单元，o为输出，L为损失函数，y为训练集的标签。这些元素右上角带的t代表t时刻的状态，其中需要注意的是，因策单元h在t时刻的表现不仅由此刻的输入决定，还受t时刻之前时刻的影响。V、W、U是权值，同一类型的权连接权值相同。</p><p>对于t时刻：<script type="math/tex">h^{(t)}  =\phi (Ux^{(t)} +Wh^{(t)} +b)</script>,其中$\phi()$为激活函数，一般来说会选择tanh函数，b为偏置。</p><p>t时刻的输出就更加简单：$o^{(t)}=Vh^{(t)}+c$。</p><p>最终模型的预测输出为：$\hat{y}^{(t)}=\sigma(o^{(t)})$其中σ为激活函数，通常RNN用于分类，所以这里一般用sotfmax函数。</p><h4 id="RNN训练方法——BPTT"><a href="#RNN训练方法——BPTT" class="headerlink" title="RNN训练方法——BPTT"></a>RNN训练方法——BPTT</h4><p>BPTT（back-propagation through time）算法是常用的训练RNN的方法，其实本质还是BP算法，只不过RNN处理时间序列数据，所以要基于时间反向传播，故叫随时间反向传播。BPTT的中心思想和BP算法相同，沿着需要优化的参数的负梯度方向不断寻找更优的点直至收敛。综上所述，BPTT算法本质还是BP算法，BP算法本质还是梯度下降法，那么求各个参数的梯度便成了此算法的核心。</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZXMyMDE1LmNuYmxvZ3MuY29tL2Jsb2cvMTA0MjQwNi8yMDE3MDMvMTA0MjQwNi0yMDE3MDMwNjE0MjI1MzM3NS0xNzU5NzE3NzkucG5n?x-oss-process=image/format,png" alt="这里写图片描述"></p><p>需要寻优的参数有三个，分别是U、V、W。与BP算法不同的是，其中W和U两个参数的寻优过程需要追溯之前的历史数据，参数V相对简单只需关注目前，那么我们就来先求解参数V的偏导数。</p><script type="math/tex; mode=display">\frac{\partial L^{(t)}}{\partial V} = \frac{\partial L^{(t)}}{\partial o^{(t)}}\cdot \frac{\partial o^{(t)}}{\partial V}</script><p>RNN的损失也是会随着时间累加的，所以不能只求t时刻的偏导。</p><script type="math/tex; mode=display">\begin{array}{c}L=\sum_{t=1}^{n} L^{(t)} \\\frac{\partial L}{\partial V}=\sum_{t=1}^{n} \frac{\partial L^{(t)}}{\partial o^{(t)}} \cdot \frac{\partial o^{(t)}}{\partial V}\end{array}</script><p>W和U的偏导的求解由于需要涉及到历史数据，其偏导求起来相对复杂，我们先假设只有三个时刻，那么<strong>在第三个时刻</strong> L对W的偏导数为：</p><script type="math/tex; mode=display">\frac{\partial L^{(3)}}{\partial W}=\frac{\partial L^{(3)}}{\partial o^{(3)}} \frac{\partial o^{(3)}}{\partial h^{(3)}} \frac{\partial h^{(3)}}{\partial W}+\frac{\partial L^{(3)}}{\partial o^{(3)}} \frac{\partial o^{(3)}}{\partial h^{(3)}} \frac{\partial h^{(3)}}{\partial h^{(2)}} \frac{\partial h^{(2)}}{\partial W}+\frac{\partial L^{(3)}}{\partial o^{(3)}} \frac{\partial o^{(3)}}{\partial h^{(3)}} \frac{\partial h^{(3)}}{\partial h^{(2)}} \frac{\partial h^{(2)}}{\partial h^{(1)}} \frac{\partial h^{(1)}}{\partial W}</script><p>相应的，L在第三个时刻对U的偏导数为：</p><script type="math/tex; mode=display">\frac{\partial L^{(3)}}{\partial U}=\frac{\partial L^{(3)}}{\partial o^{(3)}} \frac{\partial o^{(3)}}{\partial h^{(3)}} \frac{\partial h^{(3)}}{\partial U}+\frac{\partial L^{(3)}}{\partial o^{(3)}} \frac{\partial o^{(3)}}{\partial h^{(3)}} \frac{\partial h^{(3)}}{\partial h^{(2)}} \frac{\partial h^{(2)}}{\partial U}+\frac{\partial L^{(3)}}{\partial o^{(3)}} \frac{\partial o^{(3)}}{\partial h^{(3)}} \frac{\partial h^{(3)}}{\partial h^{(2)}} \frac{\partial h^{(2)}}{\partial h^{(1)}} \frac{\partial h^{(1)}}{\partial U}</script><p>可以观察到，在某个时刻的对W或是U的偏导数，需要追溯这个时刻之前所有时刻的信息，这还仅仅是一个时刻的偏导数，上面说过损失也是会累加的，那么整个损失函数对W和U的偏导数将会非常繁琐。虽然如此但好在规律还是有迹可循，我们根据上面两个式子可以写出L在t时刻对W和U偏导数的通式：</p><script type="math/tex; mode=display">\begin{array}{l}\frac{\partial L^{(t)}}{\partial W}=\sum_{k=0}^{t} \frac{\partial L^{(t)}}{\partial o^{(t)}} \frac{\partial o^{(t)}}{\partial h^{(t)}}\left(\prod_{j=k+1}^{t} \frac{\partial h^{(j)}}{\partial h^{(j-1)}}\right) \frac{\partial h^{(k)}}{\partial W} \\\frac{\partial L^{(t)}}{\partial U}=\sum_{k=0}^{t} \frac{\partial L^{(t)}}{\partial o^{(t)}} \frac{\partial o^{(t)}}{\partial h^{(t)}}\left(\prod_{j=k+1}^{t} \frac{\partial h^{(j)}}{\partial h^{(j-1)}}\right) \frac{\partial h^{(k)}}{\partial U}\end{array}</script><p>整体的偏导公式就是将其按时刻再一一加起来。</p><p>前面说过激活函数是嵌套在里面的，如果我们把激活函数放进去，拿出中间累乘的那部分：</p><script type="math/tex; mode=display">\begin{array}{c}\prod_{j=k+1}^{t} \frac{\partial h^{j}}{\partial h^{j-1}}=\prod_{j=k+1}^{t} \tanh ^{\prime} \cdot W_{s} \\\prod_{j=k+1}^{t} \frac{\partial h^{j}}{\partial h^{j-1}}=\prod_{j=k+1}^{t} \text { sigmoid }^{\prime} \cdot W_{s}\end{array}</script><p>我们会发现累乘会导致激活函数导数的累乘，进而会导致“<strong>梯度消失</strong>“和“<strong>梯度爆炸</strong>“现象的发生。</p><p>在上面式子累乘的过程中，如果取sigmoid函数作为激活函数的话，那么必然是一堆小数在做乘法，结果就是越乘越小。随着时间序列的不断深入，小数的累乘就会导致梯度越来越小直到接近于0，这就是“梯度消失“现象。其实RNN的时间序列与深层神经网络很像，在较为深层的神经网络中使用sigmoid函数做激活函数也会导致反向传播时梯度消失，梯度消失就意味消失那一层的参数再也不更新，那么那一层隐层就变成了单纯的映射层，毫无意义了，所以在深层神经网络中，有时候多加神经元数量可能会比多家深度好。</p><p>你可能会提出异议，RNN明明与深层神经网络不同，RNN的参数都是共享的，而且某时刻的梯度是此时刻和之前时刻的累加，即使传不到最深处那浅层也是有梯度的。这当然是对的，但如果我们根据<strong>有限层</strong>的梯度来更新<strong>更多层的共享的参数</strong>一定会出现问题的，因为将有限的信息来作为寻优根据必定不会找到所有信息的最优解。</p><p>之前说过我们多用tanh函数作为激活函数，那tanh函数的导数最大也才1啊，而且又不可能所有值都取到1，那相当于还是一堆小数在累乘，还是会出现“梯度消失“，那为什么还要用它做激活函数呢？原因是tanh函数相对于sigmoid函数来说梯度较大，收敛速度更快且引起梯度消失更慢。</p><p>还有一个原因是sigmoid函数还有一个缺点，Sigmoid函数输出不是零中心对称。sigmoid的输出均大于0，这就使得输出不是0均值，称为偏移现象，这将导致后一层的神经元将上一层输出的非0均值的信号作为输入。关于原点对称的输入和中心对称的输出，网络会收敛地更好。</p><p>RNN的特点本来就是能“追根溯源“利用历史数据，现在告诉我可利用的历史数据竟然是有限的，这就令人非常难受，解决“梯度消失“是非常必要的。这里说两种改善“梯度消失”的方法：<br>1、选取更好的激活函数<br>2、改变传播结构</p><p>关于第一点，一般选用ReLU函数作为激活函数</p><p>ReLU函数的左侧导数为0，右侧导数恒为1，这就避免了小数的连乘，但反向传播中仍有权值的累乘，所以说ReLU函数不能说完全解决了“梯度消失”现象，只能说改善。有研究表明，在RNN中使用ReLU函数配合将权值初始化到单位矩阵附近，可以达到接近LSTM网络的效果。但恒为1的导数容易导致“梯度爆炸“，但设定合适的阈值可以解决这个问题。还有一点就是如果左侧横为0的导数有可能导致把神经元学死，不过设置合适的步长（学习率）也可以有效避免这个问题的发生。</p><p>关于第二点，LSTM结构就是传统RNN的改善。</p><p>总结一下，sigmoid函数的缺点：<br>1、导数值范围为(0,0.25]，反向传播时会导致“梯度消失“。tanh函数导数值范围更大，相对好一点。<br>2、sigmoid函数不是0中心对称，tanh函数是，可以使网络收敛的更好。</p><h3 id="Bi-directional-Recurrent-Neural-Network-BRNN"><a href="#Bi-directional-Recurrent-Neural-Network-BRNN" class="headerlink" title="Bi-directional Recurrent Neural Network (BRNN)"></a>Bi-directional Recurrent Neural Network (BRNN)</h3><p>由于标准的循环神经网络（RNN）在时序上处理序列，他们往往忽略了未来的上下文信息。一种很显而易见的解决办法是在输入和目标之间添加延迟，进而可以给网络一些时步来加入未来的上下文信息，也就是加入M时间帧的未来信息来一起预测输出。理论上，M可以非常大来捕获所有未来的可用信息，但事实上发现如果M过大，预测结果将会变差。这是因为网路把精力都集中记忆大量的输入信息，而导致将不同输入向量的预测知识联合的建模能力下降。因此，M的大小需要手动来调节。</p><p>双向循环神经网络（BRNN）的基本思想是提出每一个训练序列向前和向后分别是两个循环神经网络（RNN），而且这两个都连接着一个输出层。这个结构提供给输出层输入序列中每一个点的完整的过去和未来的上下文信息。下图展示的是一个沿着时间展开的双向循环神经网络。六个独特的权值在每一个时步被重复的利用，六个权值分别对应：输入到向前和向后隐含层（w1, w3），隐含层到隐含层自己（w2, w5），向前和向后隐含层到输出层（w4, w6）。值得注意的是：向前和向后隐含层之间没有信息流，这保证了展开图是非循环的。</p><p>对于双向循环神经网络（BRNN）的隐含层，向前推算跟单向的循环神经网络（RNN）一样，除了输入序列对于两个隐含层是相反方向的，输出层直到两个隐含层处理完所有的全部输入序列才更新：</p><p><img src="https://img-blog.csdn.net/20160721152522777" alt="这里写图片描述"></p><p>双向循环神经网络（BRNN）的向后推算与标准的循环神经网络（RNN）通过时间反向传播相似，除了所有的输出层δ项首先被计算，然后返回给两个不同方向的隐含层：</p><p><img src="https://img-blog.csdn.net/20160721153358055" alt="这里写图片描述"></p><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>长短期记忆网络是RNN的一种变体，RNN由于梯度消失的原因只能有短期记忆，LSTM网络通过精妙的门控制将加法运算带入网络中，一定程度上解决了梯度消失的问题。只能说一定程度上，过长的序列还是会出现“梯度消失”（我记得有个老外的博客上说长度超过300就有可能出现），所以LSTM叫长一点的“短时记忆”。</p><h4 id="长期依赖（Long-Term-Dependencies）问题"><a href="#长期依赖（Long-Term-Dependencies）问题" class="headerlink" title="长期依赖（Long-Term Dependencies）问题"></a>长期依赖（Long-Term Dependencies）问题</h4><p>RNN 的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果 RNN 可以做到这个，他们就变得非常有用。但是真的可以么？答案是，还有很多依赖因素。</p><p>有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测 “the clouds are in the sky” 最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是 sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN 可以学会使用先前的信息。</p><p><img src="https://img-blog.csdnimg.cn/20190613125339273.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW9qYzE5OTU=,size_16,color_FFFFFF,t_70" alt="不太长的相关信息和位置间隔"></p><p>但是同样会有一些更加复杂的场景。假设我们试着去预测“I grew up in France… I speak fluent French”最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的 France 的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。</p><p>不幸的是，在这个间隔不断增大时，RNN 会丧失学习到连接如此远的信息的能力。</p><p><img src="https://img-blog.csdnimg.cn/20190613125403928.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW9qYzE5OTU=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="LSTM网络"><a href="#LSTM网络" class="headerlink" title="LSTM网络"></a>LSTM网络</h4><p>LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，而非需要付出很大代价才能获得的能力！</p><p>所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层。</p><p><img src="https://img-blog.csdnimg.cn/20190701145548129.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW9qYzE5OTU=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于单一神经网络层，整体上除了h在随时间流动，细胞状态c也在随时间流动，细胞状态c就代表着长期记忆。</p><p><img src="https://img-blog.csdnimg.cn/20190701145646333.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW9qYzE5OTU=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p><img src="https://img-blog.csdnimg.cn/20190701145707658.png" alt="在这里插入图片描述"></p><ul><li>黄色的矩形是学习得到的神经网络层</li><li>粉色的圆形表示一些运算操作，诸如加法乘法</li><li>黑色的单箭头表示向量的传输</li><li>两个箭头合成一个表示向量的连接</li><li>一个箭头分开表示向量的复制</li></ul><h4 id="LSTM的核心思想"><a href="#LSTM的核心思想" class="headerlink" title="LSTM的核心思想"></a>LSTM的核心思想</h4><p>LSTM 的关键就是细胞状态，水平线在图上方贯穿运行。</p><p>细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。</p><p><img src="https://img-blog.csdnimg.cn/20190701145744799.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW9qYzE5OTU=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>LSTM 有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作。</p><p>Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”！</p><p>LSTM 拥有三个门，来保护和控制细胞状态。</p><p>在我们 LSTM 中的第一步是决定我们会从细胞状态中丢弃什么信息。这个决定通过一个称为遗忘门完成。该门会读取$h<em>{t-1}$和$x_t$，输出一个在 0 到 1 之间的数值给每个在细胞状态$C</em>{t-1}$中的数字。1 表示“完全保留”，0 表示“完全舍弃”。</p><p>让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。</p><p><img src="https://img-blog.csdnimg.cn/2019070114590464.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW9qYzE5OTU=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>这里可以抛出两个问题：这个门怎么做到“遗忘“的呢？怎么理解？既然是遗忘旧的内容，为什么这个门还要接收新的$x_{t}$?<br>对于第一个问题，“遗忘“可以理解为“之前的内容记住多少“，其精髓在于只能输出（0，1）小数的sigmoid函数和粉色圆圈的乘法，LSTM网络经过学习决定让网络记住以前百分之多少的内容。对于第二个问题就更好理解，决定记住什么遗忘什么，其中新的输入肯定要产生影响。</p><p>下一步是确定什么样的新信息被存放在细胞状态中。这里包含两个部分。第一，sigmoid 层称 “输入门层” 决定什么值我们将要更新。然后，一个 tanh 层创建一个新的候选值向量，$\tilde{C}_t$会被加入到状态中。下一步，我们会讲这两个信息来产生对状态的更新。</p><p>在我们语言模型的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。</p><p><img src="https://img-blog.csdn.net/20160731214331118" alt="这里写图片描述"></p><p>现在是更新旧细胞状态的时间了，$C_{t-1}$更新为 $C_t$ 。前面的步骤已经决定了将会做什么，我们现在就是实际去完成。</p><p>我们把旧状态与$ f_t$ 相乘 ， 丢弃掉我们确定需要丢弃的信息 。接着加上相乘，丢弃掉我们确定需要丢弃的信息。接着加上相乘，丢弃掉我们确定需要丢弃的信息。接着加上 $i_t * \tilde{C}_t$。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。</p><p>有了上面的理解基础输入门，输入门理解起来就简单多了。<strong>sigmoid函数选择更新内容，tanh函数创建更新候选。</strong></p><p><img src="https://img-blog.csdn.net/20160731215529417" alt="这里写图片描述"></p><p>其中直接将新信息乘进长时记忆单元只会让情况更糟糕<strong>，导致当初c(t)=c(t-1)让导数恒为1的构想完全失效，</strong>这也说明了乘性更新并不是简单的信息叠加，而是控制和scaling。在往长时记忆单元添加信息方面，加性规则要显著优于乘性规则。<strong>也证明了加法更适合做信息叠加，而乘法更适合做控制和scaling。</strong></p><p>最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过 tanh 进行处理（得到一个在-1到1之间的值）并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。</p><p><img src="https://img-blog.csdn.net/20160731220103283" alt="这里写图片描述"></p><p>设计过程总结一下：</p><p>1、我们为了解决RNN中的梯度消失的问题，为了让梯度无损传播，想到了c(t)=c(t-1)这个朴素却没毛病的梯度传播模型，我们于是称c为“长时记忆单元”。</p><p>2、然后为了把新信息平稳安全可靠的<strong>装入</strong>长时记忆单元，我们引入了“输入门”。</p><p>3、然后为了解决新信息装载次数过多带来的<strong>激活函数饱和</strong>的问题，引入了“遗忘门”。</p><p>4、然后为了让网络能够<strong>选择合适的记忆进行输出</strong>，我们引入了“输出门”。</p><p>5、然后为了解决记忆被<strong>输出门截断</strong>后使得各个门单元<strong>受控性降低</strong>的问题，我们引入了“peephole”连接。</p><p>6、然后为了将神经网络的简单反馈结构升级成<strong>模糊历史记忆</strong>的结构，引入了隐单元h，并且发现h中存储的模糊历史记忆是短时的，于是记h为短时记忆单元。</p><p>7、于是该网络既具备长时记忆，又具备短时记忆，就干脆起名叫<strong>“长短时记忆神经网络(Long Short Term Memory Neural Networks，简称LSTM)“</strong>啦。</p><h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><p><img src="https://mmbiz.qpic.cn/mmbiz_png/o3hIBlkRCialdSxa6hONOFBkCCweB8jia5B6a5Iia0GaLnhSn8ib7C0icmtZ3gKlc5iaW71l3YAPRtaPpRtEp3W6PQ7w/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"></p><ol><li>这里GRU只有两个gate，一个是reset gate， 一个是update gate， update gate的作用类似于input gate和forget gate，(1-z)相当于input gate， z相当于forget gate。</li><li>输入为两个值，输出也为一个值，输入为输入此时时刻值x和上一个时刻的输出ht-1， 输出这个时刻的输出值ht</li><li>首先依然是利用xt和ht-1经过权重相乘通过sigmoid，得到两个0-1的值，即两个门值。</li><li>接下来这里有一些不同，并且经常容易搞混淆。对于LSTM来说依然还是xt与ht-1分别权重相乘相加，之后经过tanh函数为此时的new memory，而GRU为在这个计算过程中，在ht-1与权重乘积之后和reset gate相乘，之后最终得到new memory，这里的reset gate的作用为让这个new memory包括之前的ht-1的信息的多少。</li><li>接下来和lstm得到final memory其实一样，只是GRU只有两个输入，一个输出，其实这里h即输出也是state，就是说GRU的输出和state是一个值，所以4步骤得到的是new h，这步骤得到的是final h，通过update gate得到。</li></ol>]]></content>
    
    
    <categories>
      
      <category>神经网络</category>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RNN</tag>
      
      <tag>LSTM</tag>
      
      <tag>GRU</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>分词</title>
    <link href="/2021/01/20/%E5%88%86%E8%AF%8D/"/>
    <url>/2021/01/20/%E5%88%86%E8%AF%8D/</url>
    
    <content type="html"><![CDATA[<h4 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h4><p>分词就是把文本从字序列的表示升级为词序列表示的过程。对于中文来说，如果不进行分词，那么神经网络将直接基于原始的汉字序列进行处理和学习。然而我们知道一个字再不同的词语中可能含有不同的意思，因此我们需要分词来缓解这种一字多义的问题。</p><p>除此之外，从特征（feature）与NLP任务的角度来说，字相比词来说，是更原始和低级的特征，往往与任务目标的关联比较小；而到了词级别后，往往与任务目标能发生很强的关联。比如对于情感分类任务，“我今天走狗屎运了”这句中的每个字都跟正向情感关系不大，甚至“狗”这个字还往往跟负面情感密切相关，但是“狗屎运”这个词却表达了“幸运”、“开心”、“惊喜”的正向情感，因此，<strong>分词可以看作是给模型提供了更high-level、更直接的feature，</strong>丢给模型后自然容易获得更佳的表现。</p><p>另外，如果模型本身能够学习到字的多义性，并且学习到字组词的规律，那么就相当于隐含的内置了一个分词器再模型内部，这个时候这个内置的分词器是与解决目标任务的网络部分一起“端到端训练”的，因此甚至可能获得更好的性能。然而要满足这个条件需要训练语料非常丰富，且模型足够大，才有可能获得比“分词器+词级模型”更好的表现。</p><p>此外，<strong>分词也并不是百利而无一害的</strong>，一旦分词器的精度不够高，或者语料本身就噪声很大（错字多、句式杂乱、各种不规范用语），这时强行分词反而容易使得模型更难学习。比如模型终于学会了“哈士奇”这个词，却有人把哈士奇打成了“蛤士奇”，结果分词器没认出来，把它分成了“蛤”、“士”、“奇”这三个字，这样我们这个已经训练好的“word level模型”就看不到“哈士奇”了（毕竟模型训练的时候，“哈士奇”是基本单位）。</p><h5 id="中文分词的困难"><a href="#中文分词的困难" class="headerlink" title="中文分词的困难"></a>中文分词的困难</h5><ol><li><strong>歧义问题</strong></li><li><strong>未登录问题</strong>，比如中文词典中每年会新增一些热词，这时候分词器很容易因为“落伍”而出现切分错误。</li><li><strong>规范问题</strong>，分词时的切分边界也一直没有一个确定的规范。尽管在 1992 年国家颁布了《信息处理用现代词汉语分词规范》，但是这种规范很容易受主观因素影响，在实际场景中也难免遇到有所不及的问题。</li></ol><h4 id="常用算法"><a href="#常用算法" class="headerlink" title="常用算法"></a>常用算法</h4><h5 id="基于词典"><a href="#基于词典" class="headerlink" title="基于词典"></a>基于词典</h5><p>对于中文分词问题，最简单的算法就是基于词典直接进行<strong>greedy匹配</strong>。</p><p>比如，我们可以直接从句子开头的第一个字开始查字典，找出字典中以该字开头的最长的单词，然后就得到了第一个切分好的词。这种简单的算法即为<strong>前向最大匹配法（FMM）</strong>。</p><p>不过，<strong>由于中文句子本身具有重要信息后置的特点，从后往前匹配的分词正确率往往要高于从前往后</strong>，于是就有了反向进行的“<strong>后向最大匹配法（BMM）</strong>”。当然了，无论是FMM还是BMM，都一定存在不少切分错误，因此一种考虑更周到的方法是<strong>“双向最大匹配”</strong>。</p><h5 id="基于语言模型"><a href="#基于语言模型" class="headerlink" title="基于语言模型"></a>基于语言模型</h5><p><strong>基于词典的方法在切分时是没有考虑词语所在的上下文的，没有从全局出发找最优解</strong>。给定一个句子，各种切分组合是数量有限的，如果有一个东西可以评估出任何一个组合的存在合理性的分值，那么不就找到了最佳的分词组合嘛！</p><p>所以，这种方法的本质就是<strong>在各种切词组合中找出那个最合理的组合</strong>，这个过程就可以看作<strong>在切分词图中找出一条概率最大的路径</strong>：<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWM0LnpoaW1nLmNvbS92Mi1jYzEwNDkyNTZkZWY1YjhkMWNiY2FhOWVhM2I3OGMzZl9iLmpwZw?x-oss-process=image/format,png" alt="v2-cc1049256def5b8d1cbcaa9ea3b78c3f_b.jpg"></p><p>给定哟个句子分词后得到的单词序列{w1,w2…wm}，语言模型能够算出这个词序列存在的可能性。然后通过链式法则进行展开：</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWMxLnpoaW1nLmNvbS92Mi04ZDU4MTNiYzJiYTQyMzQzNmJiMGRkYTcxOGUzMDliMF9iLnBuZw?x-oss-process=image/format,png" alt="v2-8d5813bc2ba423436bb0dda718e309b0_b.png"></p><p>当m取值稍微一大，乘法链的后面几项会变得非常难计算（估计出这几项的概率需要依赖极其庞大的语料才能保证估计误差可接受）。计算困难怎么办？当然是<strong>用合理的假设来简化计算</strong>，比如我们可以假设当前位置取什么词仅取决于相邻的前面n个位置，即<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWM0LnpoaW1nLmNvbS92Mi1iZTNkMTIyNmRmZmFiN2Q1OWYzOWM5ZmJlYjJlZjJhYl9iLnBuZw?x-oss-process=image/format,png" alt="v2-be3d1226dffab7d59f39c9fbeb2ef2ab_b.png"></p><p>这种简化的语言模型就称为<strong>n-gram语言模型</strong>。这样乘法链中的每个乘子都可以在已经完成人工标注的分词语料中计算得到。</p><h5 id="基于统计机器学习"><a href="#基于统计机器学习" class="headerlink" title="基于统计机器学习"></a>基于统计机器学习</h5><p>一般用<strong>{B:begin, M:middle, E:end, S:single}</strong>这4个类别来描述一个分词样本中每个字所属的类别。它们代表的是该字在词语中的位置。其中，B代表该字是词语中的起始字，M代表是词语中的中间字，E代表是词语中的结束字，S则代表是单字成词。</p><p>一个样本如下所示：</p><blockquote><p>人/b 们/e 常/s 说/s 生/b 活/e 是/s 一/s 部/s 教/b 科/m 书/e</p></blockquote><p>之后我们就可以直接套用统计机器学习模型来训练出一个分词器啦。统计序列标注模型的代表就是生成式模型的代表——<strong>隐马尔可夫模型（HMM），和判别式模型的代表——（线性链）条件随机场（CRF）</strong>。</p><h5 id="基于-Bi-LSTM"><a href="#基于-Bi-LSTM" class="headerlink" title="基于(Bi-)LSTM"></a><strong>基于(Bi-)LSTM</strong></h5><p>字的上下文信息对于排解切分歧义来说非常重要，<strong>能考虑的上下文越长，自然排解歧义的能力就越强</strong>。而前面的n-gram语言模型也只能做到考虑一定距离的上下文，那么有没有在理论上能考虑无限长上下文距离的分词模型呢？答案就是<strong>基于LSTM</strong>来做。当然啦，<strong>LSTM是有方向的</strong>，为了让每个位置的字分类时既能考虑全部历史信息（左边的所有的字），又能考虑全部未来信息（右边所有的字），我们可以使用双向LSTM（Bi-LSTM）来充当序列标注的骨架模型，如图</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWMxLnpoaW1nLmNvbS92Mi04NzBkMzFlYWU0YjYyOWEzZDljZTkyNzE5ZWQ5ZmE5MF9iLmpwZw?x-oss-process=image/format,png" alt="v2-870d31eae4b629a3d9ce92719ed9fa90_b.jpg"></p><p>LSTM完成对每个位置的上下文信息的编码后，最终通过softmax分类层完成对每个位置的分类，从而跟HMM和CRF一样完成了基于序列标注的中文分词。</p><h5 id="基于预训练模型-知识蒸馏"><a href="#基于预训练模型-知识蒸馏" class="headerlink" title="基于预训练模型+知识蒸馏"></a>基于预训练模型+知识蒸馏</h5><p>BERT、ERNIE、XLNet等大型预训练席卷了NLP的绝大部分领域，在分词问题上也有显著的优越性。众所周知，预训练模型太大了，过于消耗计算资源，如果要对海量的文本进行分词，哪怕用上8卡的32G Tesla V100都会显得力不从心，因此一种解决方案就是，将预训练模型中的分词知识通过<strong>知识蒸馏（Knowledge Distillation）</strong>来迁移到小模型（比如LSTM、GRU）上。近期<strong>Jieba分词器</strong>中就上线了这么一个用这种方法得到的先进分词模型（其实是个通用的词法分析模型）。</p><h4 id="停用词过滤"><a href="#停用词过滤" class="headerlink" title="停用词过滤"></a><strong>停用词过滤</strong></h4><p>出现频率特别高的和频率特别低的词对于文本分析帮助不大，一般在预处理阶段会过滤掉。 在英文里，经典的停用词为 “The”, “an”…</p><pre><code class="hljs python"><span class="hljs-comment"># 方法1： 自己建立一个停用词词典</span>stop_words = [<span class="hljs-string">&quot;the&quot;</span>, <span class="hljs-string">&quot;an&quot;</span>, <span class="hljs-string">&quot;is&quot;</span>, <span class="hljs-string">&quot;there&quot;</span>]<span class="hljs-comment"># 在使用时： 假设 word_list包含了文本里的单词</span>word_list = [<span class="hljs-string">&quot;we&quot;</span>, <span class="hljs-string">&quot;are&quot;</span>, <span class="hljs-string">&quot;the&quot;</span>, <span class="hljs-string">&quot;students&quot;</span>]filtered_words = [word <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_list <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stop_words]<span class="hljs-keyword">print</span> (filtered_words)<span class="hljs-comment"># 方法2：直接利用别人已经构建好的停用词库</span><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> stopwordscachedStopWords = stopwords.words(<span class="hljs-string">&quot;english&quot;</span>)</code></pre><h4 id="词的标准化"><a href="#词的标准化" class="headerlink" title="词的标准化"></a><strong>词的标准化</strong></h4><p>类似于多个词代表同一个意思，我们可以用一个词来表示他们，如：went，go，going可以用go来表示所有；fast，faster，fastest可以用fast来表示所有。这些词的意思怎么合并呢？我们使用stemming来进行词的标准化。</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.stem.porter <span class="hljs-keyword">import</span> *<span class="hljs-comment">#  词标准化工具stemming（PorterStemmer()）</span>stemmer = PorterStemmer()test_strs = [<span class="hljs-string">&#x27;caresses&#x27;</span>, <span class="hljs-string">&#x27;flies&#x27;</span>, <span class="hljs-string">&#x27;dies&#x27;</span>, <span class="hljs-string">&#x27;mules&#x27;</span>, <span class="hljs-string">&#x27;denied&#x27;</span>,             <span class="hljs-string">&#x27;died&#x27;</span>, <span class="hljs-string">&#x27;agreed&#x27;</span>, <span class="hljs-string">&#x27;owned&#x27;</span>, <span class="hljs-string">&#x27;humbled&#x27;</span>, <span class="hljs-string">&#x27;sized&#x27;</span>,             <span class="hljs-string">&#x27;meeting&#x27;</span>, <span class="hljs-string">&#x27;stating&#x27;</span>, <span class="hljs-string">&#x27;siezing&#x27;</span>, <span class="hljs-string">&#x27;itemization&#x27;</span>,             <span class="hljs-string">&#x27;sensational&#x27;</span>, <span class="hljs-string">&#x27;traditional&#x27;</span>, <span class="hljs-string">&#x27;reference&#x27;</span>, <span class="hljs-string">&#x27;colonizer&#x27;</span>,             <span class="hljs-string">&#x27;plotted&#x27;</span>]<span class="hljs-comment">#  进行词的标准化</span>singles = [stemmer.stem(word) <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> test_strs]    print(<span class="hljs-string">&#x27; &#x27;</span>+ <span class="hljs-string">&quot;- &quot;</span>.join(singles)) </code></pre><h4 id="文本相似度-TF-IDF"><a href="#文本相似度-TF-IDF" class="headerlink" title="文本相似度(TF-IDF)"></a>文本相似度(TF-IDF)</h4><p>我们在比较事物时，往往会用到“不同”，“一样”，“相似”等词语，这些词语背后都涉及到一个动作——双方的比较。只有通过比较才能得出结论，究竟是相同还是不同。但是万物真的有这么极端的区分吗？在我看来不是的，生活中通过“相似度”这词来描述可能会更加准确。比如男人和女人，虽然生理器官和可能思想有些不同，但也有相同的地方，那就是都是人，就是说相似度不为0；比如石头与小草，它们对于虚拟类都是一种实体类，相似度也不为0；两个句子词和词的顺序都一致，相似度就是1。一个概念可以应用到不同于相同的两个方面的定义。可真谓方便至极了。</p><p>在生活中，信息检索、文档复制检测等领域都应用到“文本相似度”。可能有人觉得文本是文字，其实不尽然，文本相似度的应用更广，除了文字的匹配，还可以是图片，音频等，因为他们的实质都是在计算机中都是以二进制的方式存在的。</p><p>相似度，实质就是计算个体间相程度。什么是个体？对于语句，个体就是语句，对于图片，个体就是图片。</p><p>先介绍最常用最简单的方法：余弦相似度。</p><p>余弦相似度就是通过一个向量空间中两个向量夹角的余弦值作为衡量两个个体之间差异的大小。把1设为相同，0设为不同，那么相似度的值就是在01之间，所有的事物的相似度范围都应该是01，如果不是0~1的话，就不是我们应该研究的事了，那是神经学家和生物学家的事了。余弦相似度的特点是余弦值接近1，夹角趋于0，表明两个向量越相似。看下图，</p><p><img src="https://img-blog.csdnimg.cn/20201123093214761.png#pic_center" alt="在这里插入图片描述"></p><p>三角形越扁平，证明两个个体间的距离越小，相似度越大；反之，相似度越小。但是，文本的相似度计算只是针对字面量来计算的，也就是说只是针对语句的字符是否相同，而不考虑它的语义，那是另外一个研究方向来着。比如，句子1：你真好看:。句子2：你真难看。这两句话相似度75%，但是它们的语义相差十万八千里，可以说是完全相反。又比如，句子1：真好吃。句子2：很美味。两个句子相似度为0，但是语义在某个场景下是一致的。</p><p>所以在实际中，没有很完美的解决方案。每个公司会针对业务要求来调节相似度算法，使其在某些场合能够精确计算。</p><p>计算两个图片的相似度，就是把图片a，图片b，映射为向量，然后通过这个公式来计算出相似度。在这里，最最最重要的是“映射”这个过程，这个过程，如果在大数据的应用中，涉及到了对数据的分词，去重，转换，计算等步骤。</p><p><img src="https://img-blog.csdnimg.cn/20201123093259787.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JldHRlcnps,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p><img src="https://img-blog.csdnimg.cn/20201123093355798.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JldHRlcnps,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>由图可知，两个句子的相似度计算的步骤是：</p><blockquote><p>1.通过中文分词，把完整的句子根据分词算法分为独立的词集合<br>2.求出两个词集合的并集(词包)<br>3.计算各自词集的词频并把词频向量化<br>4.带入向量计算模型就可以求出文本相似度</p></blockquote><p>注意，词包确定之后，词的顺序是不能修改的，不然会影响到向量的变化。</p><p>以上是对两个句子做相似度计算，如果是对两篇文章做相似度计算，步骤如下：</p><blockquote><p>1.找出各自文章的关键词并合成一个词集合<br>2.求出两个词集合的并集(词包)<br>3.计算各自词集的词频并把词频向量化<br>4.带入向量计算模型就可以求出文本相似度</p></blockquote><p>句子的相似度计算只是文章相似度计算的一个子部分。文章的关键词提取可以通过其他的算法来实现，这里先跳过，下一篇才介绍。</p><p>到这里出现一个关键的名词——词频TF，词频是一个词语在文章或句子中出现的次数。如果一个词很重要，很明显是应该在一个文章中出现很多次的，但是这也不是绝对的，比如“地”，“的”，“啊”等词，它们出现的次数对一篇文章的中心思想没有一点帮助，只是中文语法结构的一部分而已。这类词也被称为“停用词”。所以，在计算一篇文章的词频时，停用词是应该过滤掉的。</p><p>如果某个词比较少见（在我们准备的文章库中的占比比较少），但是它在这篇文章中多次出现，那么它很可能反映了这篇文章的特性，正是我们所需要的关键词。在此，在词频TF的基础上又引出了反文档频率IDF的概念。一般来说，在一篇文章或一个句子来说，对于每个词都有不同的重要性，这也就是词的权重。在词频的基础上，赋予每一个词的权重，进一步体现该词的重要性。比如一篇报道中国农业养殖的新闻报道。最常见的词（“的”、“是”、“在”）给予最小的权重，较常见的词（“国内”、“中国”、“报道”）给予较小的权重，较少见的词（“养殖”、“维基”）。所以刻画能力强的词语，权重应该是最高的。</p><p>将TF和IDF进行相乘，就得到了一个词的TF-IDF值，某个词对文章重要性越高，该值越大，于是排在前面的几个词，就是这篇文章的关键词。（在实际中，还要考虑词的词性等多维度的特性，动词，名词，形容词的刻画能力也是有所差别的；因社会热点而词的刻画性爆发式提高(比如 打call)）。</p><p>下图是词频的计算方法：<img src="https://img-blog.csdnimg.cn/20201123093422664.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JldHRlcnps,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>词频标准化的目的是把所有的词频在同一维度上分析。词频的标准化有两个标准，第一种情况，得出词汇较小，不便于分析。一般情况下，第二个标准更适用，因为能够使词频的值相对大点，便于分析。比如一本书出现一个词语100次，但整本书10万字，词频但是在一句话中出现5次，</p><p>下面是反文档频率的计算方法：<img src="https://img-blog.csdnimg.cn/2020112309350240.png#pic_center" alt="在这里插入图片描述"></p><blockquote><p>1.为什么+1？是为了处理分母为0的情况。假如所有的文章都不包含这个词，分子就为0，所以+1是为了防止分母为0的情况。<br>2.为什么要用log函数？log函数是单调递增，求log是为了归一化，保证反文档频率不会过大。<br>3.会出现负数？肯定不会，分子肯定比分母大。</p></blockquote><p>TF-IDF = 计算的词频(TF)*计算的反文档频率(IDF)。<br>通过公式可以知道，TF-IDF与在该文档中出现的次数成正比，与包含该词的文档数成反比。</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分词</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>关键字及基本数据类型</title>
    <link href="/2021/01/19/day01/"/>
    <url>/2021/01/19/day01/</url>
    
    <content type="html"><![CDATA[<pre><code class="hljs java"><span class="hljs-comment">// 第一行的第三个单词必须和所在的文件名称完全一样，大小写也要一样</span><span class="hljs-comment">// public class后面代表定义一个类的名称，类是Java当中所有源代码的基本组织单位</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">HelloWorld</span></span>&#123;<span class="hljs-comment">//第二行是万年不变的固定写法，代表main方法</span><span class="hljs-comment">//这一行代表程序执行的起点</span><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span></span>&#123;<span class="hljs-comment">//第三行代表打印输出语句（其实就是屏幕显示）</span><span class="hljs-comment">//希望显示什么东西，就在小括号里写什么</span>System.out.println(<span class="hljs-string">&quot;HelloWorld&quot;</span>);&#125;&#125;</code></pre><h4 id="关键字特点："><a href="#关键字特点：" class="headerlink" title="关键字特点："></a>关键字特点：</h4><ol><li>完全小写的字母。</li><li>有特殊的颜色。</li></ol><h4 id="标识符"><a href="#标识符" class="headerlink" title="标识符"></a>标识符</h4><p>标识符是指在程序中我们自己定义的内容。</p><ul><li>命名规则：硬性要求<ul><li>标识符可以包含26个字母，0-9数字，$和下划线_</li><li>标识符不能以数字开头。</li><li>标识符不能是关键字。</li></ul></li><li>命名规范<ul><li>类名规范：首字母大写，后面每个单词首字母大写。</li><li>变量名规范：首字母小写，后面每个字母首字母大写。</li></ul></li></ul><h4 id="常量"><a href="#常量" class="headerlink" title="常量"></a>常量</h4><p>常量是在程序运行期间固定不变的量。</p><p>常量的分类：</p><ol><li>字符串常量，凡是用双引号亲戚来的部分都是字符串常量。</li><li>整数常量：就是整数</li><li>浮点数常量：有小数点的数字</li><li>字符常量：单引号引起来的</li><li>布尔常量：只有两种取值。true，flase。</li><li>空常量：null。代表没有任何数据。</li></ol><h4 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h4><ul><li>整数型byte short int long</li><li>浮点型 float double</li><li>字符型 char</li><li>布尔型 boolean</li><li>引用数据类型 字符串、数组、类、接口、Lambda</li><li>注意事项<ul><li>字符串不是基本类型，而是引用类型。</li><li>浮点型可能只是一个近似值，并非精确的值。</li><li>数据范围与字节数不一定相关，例如float数据范围比long更加广泛，但是float是4字节，long是8字节。</li><li>浮点数当中默认类型是double。如果一定要使用float类型，需要加上一个后缀F。</li><li>如果是整数，默认为int类型，如果一定要使用long类型，需要加上一个后缀L、推荐使用大写字母后缀。System.out.println(100);</li></ul></li></ul><h4 id="强制类型转换"><a href="#强制类型转换" class="headerlink" title="强制类型转换"></a>强制类型转换</h4><ol><li>特点：代码需要进行特殊的格式处理，不能自动完成。</li><li>格式：范围小的类型 范围小的变量名 = （范围小的类型） 原本范围大的数据；</li></ol><h5 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h5><ol><li>强制类型转换一般不推荐用，因为有可能发生精度损失，数据溢出。</li><li>byte/short/char这三种类型都可以发生数学运算，例如加法”+”</li><li>byte/short/char这三种类型在运算的时候，都会被首先提升成为int类型，然后再计算。</li><li>boolean类型不能发生数据类型转换。</li></ol><h4 id="ASCII编码表"><a href="#ASCII编码表" class="headerlink" title="ASCII编码表"></a>ASCII编码表</h4><p>数字和字符对照关系表（编码表）：</p><p>ASCII码表：American Standard Code for Information Interchange,美国信息交换标准代码。</p><p>Unicode码表：万国码。也是数字和符号的对照关系，开头0-127部分和ASCII完全一样，但是从128开始包含有更多的字符。</p><p>48-‘0’</p><p>65 - ‘A’</p><p>97-‘a’</p><h4 id="三元运算符"><a href="#三元运算符" class="headerlink" title="三元运算符"></a>三元运算符</h4><p>需要三个数据才可以操作的运算符。</p><p>格式：</p><p>数据类型 变量名称 = 条件判断?表达式A:表达式B；</p><p>流程：</p><p>首先判断条件是否成立：</p><p>​    如果成立为true，那么将表达式A的值赋值给左侧的变量。</p><p>​    如果不成立为flase，那么将表达式B的值赋值给左侧的变量。</p><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><p>定义一个方法的格式：</p><p>public static void 方法名称(){</p><p>​    方法体</p><p>}</p><h5 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h5><ol><li>方法定义的先后顺序无所谓。</li><li>方法的定义不能产生嵌套包含关系。</li><li>方法定义好之后不会执行。如果执行需要方法的调用。</li></ol>]]></content>
    
    
    <categories>
      
      <category>JAVA</category>
      
    </categories>
    
    
    <tags>
      
      <tag>JAVA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>分布式词向量</title>
    <link href="/2021/01/19/word2vec/"/>
    <url>/2021/01/19/word2vec/</url>
    
    <content type="html"><![CDATA[<h2 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h2><h4 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h4><p><img src="https://upload-images.jianshu.io/upload_images/9285151-c719e0fee3d2bcb6.png?imageMogr2/auto-orient/strip|imageView2/2/w/670/format/webp" alt="img"></p><p>word2vec本质上是只具有一个隐含层的神经元网络。它的输入是采用One-Hot编码的词汇表向量，它的输出也是One-Hot编码的词汇表向量。使用语料来训练这个神经网络直到收敛，然后从输入层与隐藏层之间的权重就是每一个词的词向量。word2vec分为CBOW和Skip-Gram两种模型。CBOW模型的训练输入时某一个特征的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。Skip-Gram模型和CBOW的思路时反着来的，即输入是特定的一个词的词向量，而输出是特定词对应的上下文的词向量。CBOW对小型数据库比较合适，而Skip-Gram再大型语料库中表现更好。</p><h4 id="二、神经概率语言模型"><a href="#二、神经概率语言模型" class="headerlink" title="二、神经概率语言模型"></a>二、神经概率语言模型</h4><p>神经概率语言模型通常包括三个层：输入层、投影层和输出层。其中W和U是权重矩阵，p和q是偏置向量。</p><p><img src="C:\Users\16355\AppData\Roaming\Typora\typora-user-images\image-20210116173055558.png" alt="image-20210116173055558"></p><p>对于语料C中的任意一个词w，将Context(w)取为其前面的n-1个词，这样对与(Context(w),w)就是一个训练样本了。</p><script type="math/tex; mode=display">\left\{\begin{array}{l}\mathbf{z}_{w}=\tanh \left(W \mathbf{x}_{w}+\mathbf{p}\right) \\\mathbf{y}_{w}=U \mathbf{z}_{w}+\mathbf{q}\end{array}\right.</script><p>这是神经网络的计算公式。假设有n-1个样本，要生成长度为m词向量，那么投影层的规模就是(n-1)m，输出层的规模就是N。然后还需要使用softmax对输出的向量归一化来表示概率。</p><h4 id="三、CBOW"><a href="#三、CBOW" class="headerlink" title="三、CBOW"></a>三、CBOW</h4><h4 id=""><a href="#" class="headerlink" title=""></a><img src="https://pic4.zhimg.com/80/v2-8fcd03fa3dad0cf4d0af1a890ace5193_720w.jpg" alt="img"></h4><p>1、输入层：上下文单词的One-Hot编码词向量，V为词汇表单词个数，C为上下文单词个数。以上文那句话为例，这里C=4，所以模型的输入是（is,an,on,the）4个单词的One-Hot编码词向量。</p><p>2、初始化一个权重矩阵 <img src="https://www.zhihu.com/equation?tex=W_%7BV%C3%97N%7D" alt="[公式]"> ，然后用所有输入的One-Hot编码词向量左乘该矩阵,得到维数为N的向量 <img src="https://www.zhihu.com/equation?tex=%CF%89_1+%CF%89_2%2C%E2%80%A6%2C%CF%89_c" alt="[公式]"> ，这里的N由自己根据任务需要设置。</p><p>3、将所得的向量 <img src="https://www.zhihu.com/equation?tex=%CF%89_1+%CF%89_2%2C%E2%80%A6%2C%CF%89_c" alt="[公式]"> 相加求平均作为隐藏层向量h。</p><p>4、初始化另一个权重矩阵 <img src="https://www.zhihu.com/equation?tex=W_%7BN%C3%97V%7D%5E%7B%27%7D" alt="[公式]"> ,用隐藏层向量h左乘 <img src="https://www.zhihu.com/equation?tex=W_%7BN%C3%97V%7D%5E%7B%27%7D" alt="[公式]"> ，再经激活函数处理得到V维的向量y，y的每一个元素代表相对应的每个单词的概率分布。</p><p>5、y中概率最大的元素所指示的单词为预测出的中间词（target word）与true label的One-Hot编码词向量做比较，误差越小越好（根据误差更新两个权重矩阵）</p><p>在训练前需要定义好损失函数（一般为交叉熵代价函数），采用梯度下降算法更新W和W’。训练完毕后，输入层的每个单词与矩阵W相乘得到的向量的就是我们想要的Distributed Representation表示的词向量，也叫做word embedding。因为One-Hot编码词向量中只有一个元素为1，其他都为0，所以第i个词向量乘以矩阵W得到的就是矩阵的第i行，所以这个矩阵也叫做look up table，有了look up table就可以免去训练过程，直接查表得到单词的词向量了。          </p><h4 id="四、Skip-gram"><a href="#四、Skip-gram" class="headerlink" title="四、Skip-gram"></a>四、Skip-gram</h4><p><img src="https://pic2.zhimg.com/80/v2-a04dca66f5e8456f50b4b43fb87b98dd_720w.jpg" alt="img"> </p><p>Skip-Gram是给定input word来预测上下文，其模型结构如上图所示。它的做法是，将一个词所在的上下文中的词作为输出，而那个词本身作为输入，也就是说，给出一个词，希望预测可能出现的上下文的词。通过在一个大的语料库训练，得到一个从输入层到隐含层的权重模型。“apple”的上下文词是（’there’，’is’，’an’，’on’,’the’,’table’）.那么以apple的One-Hot词向量作为输入，输出则是（’there’，’is’，’an’，’on’,’the’,’table’）的One-Hot词向量。训练完成后，就得到了每个词到隐含层的每个维度的权重，就是每个词的向量（和CBOW中一样）。接下来具体介绍如何训练我们的神经网络。</p><p>假如我们有一个句子“There is an apple on the table”。</p><p>1、首先我们选句子中间的一个词作为我们的输入词，例如我们选取“apple”作为input word；</p><p>2、有了input word以后，我们再定义一个叫做skip_window的参数，它代表着我们从当前input word的一侧（左边或右边）选取词的数量。如果我们设置skip_window=2，那么我们最终获得窗口中的词（包括input word在内）就是[‘is’,’an’,’apple’,’on’,’the’ ]。skip_window=2代表着选取左input word左侧2个词和右侧2个词进入我们的窗口，所以整个窗口大小span=2x2=4。另一个参数叫num_skips，它代表着我们从整个窗口中选取多少个不同的词作为我们的output word，当skip_window=2，num_skips=2时，我们将会得到两组 (input word, output word) 形式的训练数据，即 (‘apple’, ‘an’)，(‘apple’, ‘one’)。</p><p>3、神经网络基于这些训练数据中每对单词出现的次数习得统计结果，并输出一个概率分布，这个概率分布代表着到我们词典中每个词有多大可能性跟input word同时出现。举个例子，如果我们向神经网络模型中输入一个单词“中国“，那么最终模型的输出概率中，像“英国”， ”俄罗斯“这种相关词的概率将远高于像”苹果“，”蝈蝈“非相关词的概率。因为”英国“，”俄罗斯“在文本中更大可能在”中国“的窗口中出现。我们将通过给神经网络输入文本中成对的单词来训练它完成上面所说的概率计算。</p><p>4、通过梯度下降和反向传播更新矩阵W</p><p>5、W中的行向量即为每个单词的Word embedding表示</p><script type="math/tex; mode=display">$\prod_{t=1}^T \ \prod_{-m\leq j \leq m,j\neq 0} P(w^{t+j}|w^{t})$</script><p><strong>这个公式对应着两个连乘，第一个连乘是对应 T 个输入样本，也就是说我们文本中的每个单词都要做中心词。第二个连乘代表着给定一个中心词的情况下，窗口中的单词出现的概率，内含相互独立的假设</strong>。</p><h4 id="五、Hierarchical-Softmax"><a href="#五、Hierarchical-Softmax" class="headerlink" title="五、Hierarchical Softmax"></a>五、Hierarchical Softmax</h4><p>Hierarchical Softmax对原模型的改进主要有两点，第一点是从输入层到隐藏层的映射，没有采用原先的与矩阵W相乘然后相加求平均的方法，而是直接对所有输入的词向量求和。假设输入的词向量为（0，1，0，0）和（0,0,0,1），那么隐藏层的向量为（0,1,0,1）。</p><p>Hierarchical Softmax的第二点改进是采用哈夫曼树来替换了原先的从隐藏层到输出层的矩阵W’。哈夫曼树的叶节点个数为词汇表的单词个数V，一个叶节点代表一个单词，而从根节点到该叶节点的路径确定了这个单词最终输出的词向量。</p><p>​                                 <img src="https://pic4.zhimg.com/80/v2-3db7e66f36db0a9e6e6bc2f348dece47_720w.jpg" alt="img">                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   </p><h4 id="六、Negative-Sampling"><a href="#六、Negative-Sampling" class="headerlink" title="六、Negative Sampling"></a>六、Negative Sampling</h4><p>尽管哈夫曼树的引入为模型的训练缩短了许多开销，但对于一些不常见、较生僻的词汇，哈夫曼树在计算它们的词向量时仍然需要做大量的运算。</p><p>负采样是另一种用来提高Word2Vec效率的方法，它是基于这样的观察：训练一个神经网络意味着使用一个训练样本就要稍微调整一下神经网络中所有的权重，这样才能够确保预测训练样本更加精确，如果能设计一种方法每次只更新一部分权重，那么计算复杂度将大大降低。</p><p>将以上观察引入Word2Vec就是：当通过（”fox”, “quick”)词对来训练神经网络时，我们回想起这个神经网络的“标签”或者是“正确的输出”是一个one-hot向量。也就是说，对于神经网络中对应于”quick”这个单词的神经元对应为1，而其他上千个的输出神经元则对应为0。使用负采样，我们通过随机选择一个较少数目（比如说5个）的“负”样本来更新对应的权重。(在这个条件下，“负”单词就是我们希望神经网络输出为0的神经元对应的单词）。并且我们仍然为我们的“正”单词更新对应的权重（也就是当前样本下”quick”对应的神经元仍然输出为1）。</p><h2 id="fasttext"><a href="#fasttext" class="headerlink" title="fasttext"></a>fasttext</h2><p>fasttext与CBOW的模型架构很相似。在word2vec中把每个单词当作最小的单位，把每个单词都变成一个向量。这就忽略了单词的形态特征，比如”apple”和”apples”。这两个单词具有较多的公共字符，但在传统的word2vec中，这种单词的内部形态信息丢失了。为了克服这一问题，fasttext使用了字符级别的n-grams来表示一个单词。对于单词”apple”,假设n取值为3，则它的trigram有”<ap"、"app"、"ppl"、"ple"、"le>“。其中，&lt;表示前缀，&gt;表示后缀。于是，我们可以用这些trigram来表示“apple”这个单词，进一步，我们可以用这5个trigram的向量叠加来表示“apple”的词向量。</p><p>这带来两点<strong>好处</strong>：</p><ol><li><p>对于低频词生成的词向量效果会更好。因为它们的n-gram可以和其它词共享。</p></li><li><p>对于训练词库之外的单词，仍然可以构建它们的词向量。我们可以叠加它们的字符级n-gram向量。</p></li></ol><h4 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h4><p><img src="C:\Users\16355\AppData\Roaming\Typora\typora-user-images\image-20210118141225474.png" alt="image-20210118141225474"></p><p>fastText模型只有三层：输入层、隐含层、输出层，输入也是向量表示的单词，输出也是一个特定的target，隐含层是对多个词向量的叠加平均。不过CBOW的输入是目标单词的上下文，并且通过one-hot编码过。但fasttext的输入是对个单词及其n-gram特征，并且被embedding。值得注意的是，fastText在输入时，将单词的字符级别的n-gram向量作为额外的特征；在输出时，fastText采用了分层Softmax，大大降低了模型训练时间。这两个知识点在前文中已经讲过，这里不再赘述。</p><p>fastText的核心思想就是：将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。这中间涉及到两个技巧：字符级n-gram特征的引入以及分层Softmax分类。</p><h2 id="GloVe"><a href="#GloVe" class="headerlink" title="GloVe"></a>GloVe</h2><p>glove模型也是一种转化为词向量的工具。首先基于语料库构建共现矩阵然后基于共现矩阵和glove模型学习词向量。</p><h4 id="统计共现矩阵"><a href="#统计共现矩阵" class="headerlink" title="统计共现矩阵"></a>统计共现矩阵</h4><p>设共现矩阵为X，其元素为Xi<em>,</em>j</p><p><em>Xi</em>,<em>j</em>的意义为：在整个语料库中，单词i和单词j共同出现在一个窗口中的次数。</p><p>比如一段话i love you but you love him i am sad，涉及到七个单词i、love、you、but、him、am、sad。</p><p>如果采用窗口宽度为5，左右为2的统计窗口，那么就有9个窗口。以中心词为love，语境词为but、you、him、i的窗口为例子。则</p><script type="math/tex; mode=display">X_love,but += 1\\X_love,you += 1\\X_love,him += 1\\X_love,i += 1\\</script><p>使用窗口将整个语料库遍历一遍，即可得到共现矩阵X。</p><p><img src="https://img2018.cnblogs.com/blog/1338991/201907/1338991-20190718000306405-461485887.png" alt="img"></p><p><img src="https://img2018.cnblogs.com/blog/1338991/201907/1338991-20190718000335985-604228384.png" alt="img"></p><p><img src="https://img2018.cnblogs.com/blog/1338991/201907/1338991-20190718000409375-37595833.png" alt="img"></p><p><img src="https://img2018.cnblogs.com/blog/1338991/201907/1338991-20190718000553238-2003865637.png" alt="img"></p><p><img src="https://img2018.cnblogs.com/blog/1338991/201907/1338991-20190718000617760-903171449.png" alt="img"></p><h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p>[1] <a href="https://blog.csdn.net/itplus/article/details/37999613">https://blog.csdn.net/itplus/article/details/37999613</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/61635013">https://zhuanlan.zhihu.com/p/61635013</a></p><p>[3]<a href="https://blog.csdn.net/u014665013/article/details/79642083">https://blog.csdn.net/u014665013/article/details/79642083</a></p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>word2vec</tag>
      
      <tag>词向量</tag>
      
      <tag>fasttext</tag>
      
      <tag>clove</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图</title>
    <link href="/2020/10/17/%E5%9B%BE/"/>
    <url>/2020/10/17/%E5%9B%BE/</url>
    
    <content type="html"><![CDATA[<h3 id="图"><a href="#图" class="headerlink" title="图"></a>图</h3><p>图表示多对多地关系。包含</p><ul><li>一组顶点：通常用V表示顶点集合。</li><li>一组边：通常用E表示边的集合。<ul><li>边是顶点对：（v,w）∈E，其中v，w∈V</li><li>有向边<v,w>表示从v指向w的边（单行线）</li><li>不考虑重边和自回路</li></ul></li></ul><h5 id="常见术语"><a href="#常见术语" class="headerlink" title="常见术语"></a>常见术语</h5><p>图中所有路径都是右方向的称为无向图，有方向的称为有向图。</p><p>顶点的度指顶点相连接的边的条数。在有向图中，入度表示有多少条边指向这个顶点。出度表示有多少条边是以这个顶点为起点指向其他顶点。带权重的图称为网络。</p><h5 id="图的表示"><a href="#图的表示" class="headerlink" title="图的表示"></a>图的表示</h5><h6 id="邻接矩阵"><a href="#邻接矩阵" class="headerlink" title="邻接矩阵"></a>邻接矩阵</h6><ul><li>使用二维数组来存储，若有边为1，无边则为0。如果用一个长度为N(N+1)/2的1维数组A存储，则Gij在A中对应的下标是(i*(i+1)/2+j)</li><li>好处<ul><li>直观、简单、好理解</li><li>方便检查任意一对顶点间是否存在边</li><li>方便找任一顶点的所有“邻接点”（有边直接相连的顶点）</li><li>方便计算任一顶点的“度”（从该点发出的边数为“出度”，指向该点的边数为“入度”）<ul><li>无向图：对应行非0元素的个数</li><li>有向图：对应行非0元素的个数是“出度”；对应列非0元素的个数是“入度”。</li></ul></li></ul></li><li>坏处<ul><li>浪费空间，存稀疏图有大量无效元素</li></ul></li></ul><h6 id="邻接表"><a href="#邻接表" class="headerlink" title="邻接表"></a>邻接表</h6><ul><li>G[N]维指针数组，对应矩阵每行一个链表，只存非0元素。</li><li>好处<ul><li>方便找任一顶点的所有邻接点</li><li>节约稀疏图的空间</li><li>对于无向图方便计算任一顶点的度，对于有向图只能计算出度，需要构造逆邻接表来方便计算入度。</li><li>不方便检查任意一堆顶点间是否存在边。 </li></ul></li></ul><h3 id="图的遍历"><a href="#图的遍历" class="headerlink" title="图的遍历"></a>图的遍历</h3><p>深度优先搜索(DFS)相当于树的先序遍历。<br>广度优先搜索相当于树的层序遍历。，它其实就是一种“地毯式”层层推进的搜索策略，即先查找离起始顶点最近的，然后 是次近的，依次往外搜索。</p><p>广度优先搜索，通俗的理解就是，地毯式层层推进，从起始顶点开始，依次往外遍历。广度优先搜索需要借助队列来实现，遍历得到的路径就是，起始顶点到终 止顶点的最短路径。深度优先搜索用的是回溯思想，非常适合用递归实现。换种说法，深度优先搜索是借助栈来实现的。在执行效率方面，深度优先和广度优先 搜索的时间复杂度都是O(E)，空间复杂度是O(V)。 </p><p> 最短路径问题的抽象<br> 在网络中，求两个不同顶点之间的所有路径中，边的权值之和最小的那一条路径。这条路径就是两点之间的最短路径，第一个顶点为源点，最后一个顶点为终点。</p><p> 无权图的单源最短路算法<br> 按照递增的顺序找出到各个顶点的最短路。相当于BFS。 </p>]]></content>
    
    
    <categories>
      
      <category>数据结构</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>哈夫曼树和哈夫曼编码</title>
    <link href="/2020/10/11/%E5%93%88%E5%A4%AB%E6%9B%BC%E6%A0%91%E5%92%8C%E5%93%88%E5%A4%AB%E6%9B%BC%E7%BC%96%E7%A0%81/"/>
    <url>/2020/10/11/%E5%93%88%E5%A4%AB%E6%9B%BC%E6%A0%91%E5%92%8C%E5%93%88%E5%A4%AB%E6%9B%BC%E7%BC%96%E7%A0%81/</url>
    
    <content type="html"><![CDATA[<h4 id="哈夫曼树"><a href="#哈夫曼树" class="headerlink" title="哈夫曼树"></a>哈夫曼树</h4><p>带权路径长度(WPL):设二叉树有n个叶子结点，每个叶子结点带有权值wk，从根结点到每个叶子结点的长度为lk，则每个叶子结点的带权路径长度之和就是每个的路径和权重相乘之和。</p><p>最优化二叉树或哈夫曼树：wpl最小二叉树。</p><h4 id="哈夫曼树的构造"><a href="#哈夫曼树的构造" class="headerlink" title="哈夫曼树的构造"></a>哈夫曼树的构造</h4><p>每次把权值最小的两棵二叉树合并。</p><pre><code class="hljs rust">typedef <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">TreeNode</span></span> *HuffmanTree;<span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">TreeNode</span></span>&#123;    int Weight;    HuffmanTree Left,Right;&#125;HuffmanTree Huffman(MinHeap H)&#123;    <span class="hljs-comment">//假设H-&gt;Size个权值已经存在H-&gt;Elements[]-&gt;Weight里</span>    int i;HuffmanTree T;    BuildMinHeap(H);  <span class="hljs-comment">//将H-&gt;Elements[]按权值调整为最小堆</span>    <span class="hljs-keyword">for</span>(i=<span class="hljs-number">1</span>;i&lt;H-&gt;size;i++)&#123; <span class="hljs-comment">//做H-&gt;Size-1次合并</span>    T = malloc(sizeof(<span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">TreeNode</span></span>)); <span class="hljs-comment">//建立新结点    </span>    T-&gt;Left = DeleteMin(H); <span class="hljs-comment">//从最小堆中删除一个结点，作为新T的左子结点</span>    T-&gt;Right = DeleteMin(H);    <span class="hljs-comment">//从最小堆中删除一个结点，作为新T的右子结点</span>    T-&gt;Weight = T-&gt;Left-&gt;Weight+T-&gt;Right-&gt;Weight;   <span class="hljs-comment">//计算新权值</span>    Insert(H,T);  <span class="hljs-comment">//将新T插入最小堆</span>    &#125;    T = DeleteMin(H);    <span class="hljs-keyword">return</span> T;&#125;</code></pre><h4 id="哈夫曼树的特点"><a href="#哈夫曼树的特点" class="headerlink" title="哈夫曼树的特点"></a>哈夫曼树的特点</h4><ul><li>没有度为1的结点；</li><li>n个叶子结点的哈夫曼树共有2n-1个节点；</li><li>哈夫曼树的任意非叶节点的左右子树交换后仍然是哈夫曼树；</li><li>对同一组权值，可能存在不同构的两颗哈夫曼树</li></ul><p>哈夫曼树为了让字符串的编码存储空间最小，就是不等长编码。为了避免二义性可以无二义地解码，我们可以用二叉树进行编码。即左右分支为0、1，字符只在叶结点上。所以让字符编码存储空间最小就等价于构造哈夫曼树。</p>]]></content>
    
    
    <categories>
      
      <category>数据结构</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>树和堆</title>
    <link href="/2020/10/10/%E6%A0%91%E5%92%8C%E5%A0%86/"/>
    <url>/2020/10/10/%E6%A0%91%E5%92%8C%E5%A0%86/</url>
    
    <content type="html"><![CDATA[<h1 id="树"><a href="#树" class="headerlink" title="树"></a>树</h1><p>树是由n个结点构成的有限集合。当n=0时称为空树。</p><p> 对于任一棵非空树，树中有一个称为“根（root）”的特殊结点，用r表示。其余结点可以分为m个互不相交的有限集，其中每个集合本身又是一棵树，称为原来树的子树。子树是不相交的，除了根结点外，每个结点有且仅有一个父结点，一棵N个结点的树有N-1条边。</p><p> 二叉树的定义：一个有穷的结点集合。这个集合可以为空。若不为空，则它是由根结点和称为其左子树和右子树的两个不相交的二叉树组成。</p><p> 对任何非空二叉树T，若n0表示叶结点的个数，n2表示度为2的非叶节点个数，那么两者满足关系n0=n2+1。</p><h3 id="二叉树的存储结构"><a href="#二叉树的存储结构" class="headerlink" title="二叉树的存储结构"></a>二叉树的存储结构</h3><p>1.顺序存储结构，完全二叉树，从上至下，从左到右顺序存储n个结点的完全二叉树的结点父子关系。对于非根节点的父节点的序号是i/2.结点（序号为i）的左孩子节点的序号是2i，（若2i&lt;=n，否则没有左孩子）。结点(序号为i)的右孩子节点的序号是2i+1（若2i+1&lt;=n,否则没有右孩子)。<br>2.链式存储结构，就是把结点分成左孩子指针和兄弟指针。</p><h3 id="二叉树的遍历"><a href="#二叉树的遍历" class="headerlink" title="二叉树的遍历"></a>二叉树的遍历</h3><p>1.先序遍历，遍历过程为访问根结点，先序遍历其左子树，先序遍历其右子树。<br>2.中序遍历，遍历过程为中序遍历其左子树，访问根结点，然后中序遍历其右子树。<br>3.后序遍历，遍历过程为后序遍历其左子树，后序遍历其右子树最后访问根结点。</p><p>先序、中序和后序遍历过程：遍历过程中经过结点的路线一样，只是访问各节点的时机不同。</p><p>二叉树中序遍历非递归遍历算法，使用非递归算法实现 。遇到一个结点就把他压栈，并且取遍历他的左子树。当左子树遍历结束后，从栈顶弹出这个节点并且访问它，然后按其右指针再去中序遍历该节点的右子树。</p><pre><code class="hljs gauss">void <span class="hljs-built_in">InOrderTraversal</span>(BinTree BT)&#123;    BinTree T=BT;    Stack S = <span class="hljs-built_in">CreatStack</span>(MaxSize);<span class="hljs-comment">//创建并初始化堆栈S</span>    <span class="hljs-keyword">while</span>(T|| !<span class="hljs-built_in">IsEmpty</span>(s))&#123;        <span class="hljs-keyword">while</span>(T)&#123;   <span class="hljs-comment">//一直向左并将沿途结点压入堆栈</span>            <span class="hljs-keyword">Push</span>(S,T);            T = T-&gt;Left;        &#125;        <span class="hljs-keyword">if</span>(!<span class="hljs-built_in">IsEmpty</span>(s))&#123;            T = <span class="hljs-keyword">Pop</span>(S); <span class="hljs-comment">//结点弹出堆栈</span>            <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%5d&quot;</span>,T-&gt;Data);  <span class="hljs-comment">//打印节点</span>            T = T-&gt;Right;   <span class="hljs-comment">//转向右子树</span>        &#125;    &#125;&#125;</code></pre><h5 id="二叉树的层序遍历"><a href="#二叉树的层序遍历" class="headerlink" title="二叉树的层序遍历"></a>二叉树的层序遍历</h5><p>二叉树遍历的核心问题：二维结构的线性化。层序基本过程：先根结点入队，然后从队列中取出一个元素，访问该元素所指结点，若该元素所指结点的左右孩子结点为空，则将其左右孩子的指针顺序入队。</p><pre><code class="hljs excel">void LevelOrderTraversal(BinTree BT)&#123;    Queue Q; BinTree <span class="hljs-built_in">T</span>;    <span class="hljs-built_in">if</span> (!BT) return;    Q = CreateQueue(MaxSize);    AddQ(Q，BT)；    while(!IsEmptyQ(Q))&#123;        <span class="hljs-built_in">T</span> = DeleteQ(Q);        printf(<span class="hljs-string">&quot;%d\n&quot;</span>,<span class="hljs-built_in">T</span>-&gt;data);        <span class="hljs-built_in">if</span>(<span class="hljs-built_in">T</span>-&gt;<span class="hljs-built_in">Left</span>) AddQ(Q,<span class="hljs-built_in">T</span>-&gt;<span class="hljs-built_in">Left</span>);        <span class="hljs-built_in">if</span>(<span class="hljs-built_in">T</span>-&gt;<span class="hljs-built_in">Right</span>) AddQ(Q,<span class="hljs-built_in">T</span>-&gt;<span class="hljs-built_in">Right</span>);    &#125;&#125;</code></pre><p>先序和中序遍历序列来确定一棵二叉树，先根据先序遍历序列第一个结点确定根结点，然后根据根结点在中序遍历序列中分割出左右两个子序列，最后对左子树和右子树分别递归使用相同的方法继续分解。</p><h3 id="二叉搜索树的查找操作"><a href="#二叉搜索树的查找操作" class="headerlink" title="二叉搜索树的查找操作"></a>二叉搜索树的查找操作</h3><p>查找从根结点开始，如果树为空，返回NULL。若搜索数为空，则根结点关键字和X进行比较，并进行不同处理。若X小于根结点键值，只需在左子树中继续搜索，如果X大于根结点的键值，在右子树中继续搜索，如果两者比较结果是相等的，则搜索完成，返回指向此结点的指针。</p><p>二叉搜索树的插入，关键是要找到元素应该插入的位置。 </p><pre><code class="hljs coq">BinTree Insert(ElementType X,BinTree BST)&#123;    <span class="hljs-keyword">if</span>(!BST)&#123;        //若原树为空，生成并返回一个结点的二叉搜索树        BST = malloc(sizeof(struct TreeNode));        BST-&gt;Data = X;        BST-&gt;<span class="hljs-keyword">Left</span> = BST-&gt;<span class="hljs-keyword">Right</span> = NULL;    &#125;<span class="hljs-keyword">else</span> //开始找要插入元素的位置        <span class="hljs-keyword">if</span>(x&lt;BST-&gt;Data)            BST-&gt;<span class="hljs-keyword">Left</span> = Insert(x,BST-&gt;<span class="hljs-keyword">Left</span>);            //递归插入左子树        <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(x&gt;BST-&gt;Data)            //递归插入右子树            BST-&gt;<span class="hljs-keyword">Right</span> = Insert(x,BST-&gt;<span class="hljs-keyword">Right</span>);        //<span class="hljs-keyword">else</span> x已经存在，什么都不做    <span class="hljs-keyword">return</span> BST;&#125;</code></pre><h3 id="二叉搜索树的删除"><a href="#二叉搜索树的删除" class="headerlink" title="二叉搜索树的删除"></a>二叉搜索树的删除</h3><p> 要删除的结点只有一个孩子结点，将其父结点的指针指向要删除结点的孩子结点</p><p>要删除的结点有左右两棵子树，用另一结点替代被删除结点：右子树的最小元素或者左子树的最大元素。</p><pre><code class="hljs coq">BinTree Delete(ElementType X,BinTree BST)&#123;    Position Tmp;    <span class="hljs-keyword">if</span>(!BST)printf(<span class="hljs-string">&quot;要删除的元素未找到&quot;</span>);    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(X&lt;BST-&gt;Data)            BST-&gt;<span class="hljs-keyword">Left</span> = Delete(X,BST-&gt;<span class="hljs-keyword">Left</span>);    //左子树递归删除    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(x&gt;BST-&gt;Data)            BST-&gt;<span class="hljs-keyword">Left</span> = Delete(X,BST-&gt;<span class="hljs-keyword">Right</span>);   //右子树递归删除    <span class="hljs-keyword">else</span> //找到要删除的结点        <span class="hljs-keyword">if</span>(BST-&gt;<span class="hljs-keyword">Left</span> &amp;&amp; BST-&gt;<span class="hljs-keyword">Right</span>)&#123; //被删除结点有左右两个子节点            Tmp = FindMin(BST-&gt;<span class="hljs-keyword">Right</span>);  //在右子树中找最小的元素填充删除的结点            BST-&gt;Data = Tmp-&gt;Data;            BST-&gt;<span class="hljs-keyword">Right</span> = Delete(BST-&gt;Data,BST-&gt;<span class="hljs-keyword">Right</span>);                                            //在删除结点的右子树中删除最小元素        &#125;<span class="hljs-keyword">else</span>&#123;  //被删除结点有一个或无子结点            Tmp = BST;            <span class="hljs-keyword">if</span>(!BST-&gt;<span class="hljs-keyword">Left</span>) //有右孩子或无子结点                BST = BST-&gt;<span class="hljs-keyword">Right</span>;            <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(!BST-&gt;<span class="hljs-keyword">Right</span>)                BST = BST-&gt;<span class="hljs-keyword">Left</span>;            free(Tmp);        &#125;    <span class="hljs-keyword">return</span> BST; &#125;</code></pre><h3 id="平衡二叉树（avl树）"><a href="#平衡二叉树（avl树）" class="headerlink" title="平衡二叉树（avl树）"></a>平衡二叉树（avl树）</h3><p>BF(T) = hL-hR,其中hL和hR分别为T的左、右子树的高度。<br>空树或者任一结点左右子树高度差的绝对值不超过1，即|BF(T)|&lt;=1</p><h1 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h1><p>优先级队列，顾名思义，它首先应该是一个队列。我们前面讲过，队列最大的特性就是先进先出。不过，在优先级队列中，数据的出队顺序不是先进先出，而是 按照优先级来，优先级最高的，最先出队。 如何实现一个优先级队列呢？方法有很多，但是用堆来实现是最直接、最高效的。这是因为，堆和优先级队列非常相似。一个堆就可以看作一个优先级队列。很 多时候，它们只是概念上的区分而已。往优先级队列中插入一个元素，就相当于往堆中插入一个元素；从优先级队列中取出优先级最高的元素，就相当于取出堆 顶元素。 </p><p>堆是一种完全二叉树。它最大的特性是：每个节点的值都大于等于（或小于等于）其子树节点的值。因此，堆被分成了两类，大 顶堆和小顶堆。 堆中比较重要的两个操作是插入一个数据和删除堆顶元素。这两个操作都要用到堆化。插入一个数据的时候，我们把新插入的数据放到数组的最后，然后从下往 上堆化；删除堆顶数据的时候，我们把数组中的最后一个元素放到堆顶，然后从上往下堆化。这两个操作时间复杂度都是logn。</p><h3 id="堆的两个特性"><a href="#堆的两个特性" class="headerlink" title="堆的两个特性"></a>堆的两个特性</h3><p>结构性：用数组表示完全二叉树。<br>有序性：任一结点的关键字是其子树所有节点的最大值。最大堆，也称大顶堆：最大值。最小堆也称小顶堆：最小值。</p><h3 id="最大堆的创建"><a href="#最大堆的创建" class="headerlink" title="最大堆的创建"></a>最大堆的创建</h3><pre><code class="hljs arduino"><span class="hljs-keyword">typedef</span> <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">HeapStruct</span> &quot;<span class="hljs-title">MaxHeap</span>&quot;;</span><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">HeapStruct</span>&#123;</span>        ElementType *Elements; <span class="hljs-comment">//存储堆元素的数组</span>        <span class="hljs-keyword">int</span> Size;   <span class="hljs-comment">//堆的当前元素个数</span>        <span class="hljs-keyword">int</span> Capacity; <span class="hljs-comment">//堆的最大容量</span>&#125;;<span class="hljs-function">MaxHeap <span class="hljs-title">Create</span><span class="hljs-params">(<span class="hljs-keyword">int</span> MaxSize)</span></span>&#123;    <span class="hljs-comment">//创建容量为MaxSize的空的最大堆</span>    MaxHeap H = <span class="hljs-built_in">malloc</span>(<span class="hljs-keyword">sizeof</span>(struct HeapStruct));    H-&gt;Elements = <span class="hljs-built_in">malloc</span>((MaxSize+<span class="hljs-number">1</span>)*<span class="hljs-keyword">sizeof</span>(ElementType));    H-&gt;<span class="hljs-built_in">size</span> = <span class="hljs-number">0</span>;    H-&gt;Capacity = MaxSize;    H-&gt;Elements[<span class="hljs-number">0</span>] = MaxData;        <span class="hljs-comment">//定义哨兵为大于堆中所有可能元素的值，便于以后更快操作</span>    <span class="hljs-keyword">return</span> H;&#125;</code></pre><p>将新增节点插入到其父节点到根结点的有序序列中<br><pre><code class="hljs c"><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">Insert</span><span class="hljs-params">(MaxHeap H,ElementsType item)</span></span>&#123;    <span class="hljs-comment">//将元素item插入最大堆H，其中H-&gt;Elements[0]已经定义为哨兵</span>    <span class="hljs-keyword">int</span> i;    <span class="hljs-keyword">if</span>(IsFull(H))&#123;        <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;最大堆已满&quot;</span>);        <span class="hljs-keyword">return</span> ;    &#125;    i = ++H-&gt;Size;  <span class="hljs-comment">//i指向插入后堆中的最后一个元素位置</span>    <span class="hljs-keyword">for</span>(;H-&gt;Elements[i/<span class="hljs-number">2</span>] &lt; item;i/<span class="hljs-number">2</span>)        H-&gt;Elements[i] = H-&gt;Elements[i/<span class="hljs-number">2</span>]; <span class="hljs-comment">//向下过滤节点</span>    H-&gt;Elements[i] = item;  <span class="hljs-comment">//将item插入</span>    &#125;</code></pre></p>]]></content>
    
    
    <categories>
      
      <category>数据结构</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>复杂度分析</title>
    <link href="/2020/09/27/%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%88%86%E6%9E%90/"/>
    <url>/2020/09/27/%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<p>讲在之前要说一下几种记号。</p><ul><li>大O记号，大O记号描述了一个算法最坏的情况。因为是假设n&gt;&gt;2,从而通过n的替换来放大函数（常系数可以忽略，低次项可以忽略），并且相比于原式更加简洁也可以反映前者的增长趋势。这也是我们通常参考的时间复杂度。</li><li>大Ω记号，大Ω记号描述算法最好的情况，也是假设n&gt;&gt;2，从而通过n的替换来缩小函数，这通常是函数的下界。</li><li>大θ记号，大θ来描述算法的确切情况，是介于大O和大Ω之间的。</li></ul><hr><p>推导大O阶：<br>1.用常数1取代运行时间中的所有加法常数。<br>2.在修改后的运行次数函数中，只保留最高阶项。<br>3.弱国最高阶项存在且不是1，则去除与这个项相乘的常数。得到的就是大O阶。</p><p>一个程序的时间复杂度大大影响了程序的时间，所以当我们在做一些有运行时间限制的算法题的时候，要优先考虑代码的时间复杂度。</p><p>下面列举几种常见的==时间复杂度==</p><h4 id="gt-1、常数复杂度O-1"><a href="#gt-1、常数复杂度O-1" class="headerlink" title="&gt; 1、常数复杂度O(1)"></a>&gt; 1、常数复杂度O(1)</h4><p>只执行一次的代码。一般情况不含转向（循环、调用、递归等），是循序执行的即是O(1)</p><pre><code class="hljs angelscript"><span class="hljs-built_in">array</span> = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>]a = <span class="hljs-built_in">array</span>[<span class="hljs-number">0</span>]</code></pre><h4 id="gt-2、线性复杂度o-n"><a href="#gt-2、线性复杂度o-n" class="headerlink" title="&gt; 2、线性复杂度o(n)"></a>&gt; 2、线性复杂度o(n)</h4><p>只有一层循环。例子图的遍历，二叉树遍历(n代表二叉树节点总数，因为每个节点访问一次，且仅访问一次)、二维数组二分法查找（有序）。还有DFS和BFS。</p><pre><code class="hljs angelscript"><span class="hljs-keyword">for</span>(<span class="hljs-built_in">int</span> i = <span class="hljs-number">0</span>;i&lt;<span class="hljs-number">3</span>;i++);</code></pre><h4 id="gt-3、n平方复杂度O-n-2"><a href="#gt-3、n平方复杂度O-n-2" class="headerlink" title="&gt; 3、n平方复杂度O(n^2)"></a>&gt; 3、n平方复杂度O(n^2)</h4><p>双层嵌套循环</p><pre><code class="hljs angelscript"><span class="hljs-keyword">for</span>(<span class="hljs-built_in">int</span> i = <span class="hljs-number">0</span>;i&lt;<span class="hljs-number">3</span>;i++);    <span class="hljs-keyword">for</span>(<span class="hljs-built_in">int</span> n = <span class="hljs-number">0</span>;n&lt;<span class="hljs-number">3</span>;n-- );</code></pre><h4 id="gt-4、指数复杂度O-2-n"><a href="#gt-4、指数复杂度O-2-n" class="headerlink" title="&gt; 4、指数复杂度O(2^n)"></a>&gt; 4、指数复杂度O(2^n)</h4><p>费伯罗契数列</p><pre><code class="hljs routeros"><span class="hljs-comment">#include &lt;stdio.h&gt;</span>void main()&#123;int <span class="hljs-attribute">a</span>=0;int <span class="hljs-attribute">b</span>=1;int <span class="hljs-attribute">c</span>=0;int i;<span class="hljs-keyword">for</span>(<span class="hljs-attribute">i</span>=0;i&lt;10000;i++)&#123;    <span class="hljs-attribute">a</span>=b;    <span class="hljs-attribute">b</span>=c;    <span class="hljs-attribute">c</span>=a+b;    printf(<span class="hljs-string">&quot;%d&quot;</span>,c);&#125;&#125;</code></pre><h4 id="gt-5、对数复杂度O-logn"><a href="#gt-5、对数复杂度O-logn" class="headerlink" title="&gt; 5、对数复杂度O(logn)"></a>&gt; 5、对数复杂度O(logn)</h4><p>对于对数复杂度，底数不再要求。因为底数是常数，只需要根据对数运算法则，底数可任意变换，无非是前面多了一个常数系数。而在时间复杂度计算系数是可以忽略的。那么同理也有常数次幂无所谓。此类算法是非常高效的，复杂度无限接近于常数。</p><p>例如：一维数组二分查找<br>另外归并排序的时间复杂度是O(nlogn)</p><h4 id="gt-6、阶乘复杂度"><a href="#gt-6、阶乘复杂度" class="headerlink" title="&gt; 6、阶乘复杂度"></a>&gt; 6、阶乘复杂度</h4><p>例子求阶乘</p><hr><h2 id="空间复杂度"><a href="#空间复杂度" class="headerlink" title="==空间复杂度=="></a>==空间复杂度==</h2><p>空间复杂度可以节省缓存空间，节约内存。</p><p>1、数组的长度基本决定了空间复杂度，对于一维数组的长度就是空间复杂度O(n)。二维数组的长度为n^2,那空间复杂度为O(n^2)。</p><p>2、递归的深度也表示空间复杂度的最大值，若递归里包含数组，那么数组和递归两者之间的最大值为空间复杂度。<br>3、算法的空间复杂度通过计算算法所需的存储空间实现，算法空间复杂度的计算公式记作：S（n）=O（f（n）），其中，n为问题的规模，F（n）为语句关于n所占存储空间的函数。</p>]]></content>
    
    
    <categories>
      
      <category>数据结构</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>线性表、栈和队列</title>
    <link href="/2020/09/27/%E7%BA%BF%E6%80%A7%E8%A1%A8/"/>
    <url>/2020/09/27/%E7%BA%BF%E6%80%A7%E8%A1%A8/</url>
    
    <content type="html"><![CDATA[<h4 id="线性表"><a href="#线性表" class="headerlink" title="线性表"></a>线性表</h4><p>线性表指零个或多个数据元素的有限序列。<br>在非空表中每个数据元素都有一个确定的位置，如a1是第一个数据元素，an是最后一个数据元素，ai是第i个数据元素，称i为数据元素ai在线性表中的次序。在较复杂的线性表中，一个数据元素可以由若干个数据项组成。</p><p>线性表的链式存储结构的特点是用一组任意的存储单元存储线性表的数据元素，这组存储单元可以存在内存中未被占用的任意位置。</p><p>比起顺序存储结构每个数据元素只需要存储一个位置就可以了。现在链式存储结构中，除了要存储数据元素信息外，还要存储它的后继元素的存储地址。</p><p>我们把存储数据元素信息的领称为数据域，把存储直接后继位置的域称为指针域。指针域中存储的信息称为指针或链。这两部分信息组成数据元素称为存储映像，称为结点（node）。</p><p>我们把链表中的第一个结点的存储位置叫做头指针，最后一个结点为空。头结点的数据域一般不存储任何信息。</p><h5 id="头指针"><a href="#头指针" class="headerlink" title="头指针"></a>头指针</h5><ul><li>头指针是指链表比指向第一个结点的指针，若链表有头结点，则是指向头结点的指针。</li><li>头指针具有标识作用，所以常用头指针冠以链表的名字（指针变量的名字）。</li><li>无论链表是否为空，头指针均不为空。</li><li>头指针是链表的必要元素。<h5 id="头结点"><a href="#头结点" class="headerlink" title="头结点"></a>头结点</h5></li><li>头结点是为了操作的统一和方便而设立的，放在第一个元素的结点之前，其数据域一般无意义（但也可以用来存放链表的长度）。</li><li>有了头结点，对在第一元素结点前插入结点和删除第一结点起操作与其他结点的操作统一了。</li><li>头结点不一定是链表的必要元素。 </li></ul><h5 id="头插法建立单链表"><a href="#头插法建立单链表" class="headerlink" title="头插法建立单链表"></a>头插法建立单链表</h5><p>头插法从一个空表开始，生成新结点，读取数据存放到新结点的数据域中，然后将新结点插入到当前链表的表头上，直到结束为止。 </p><h5 id="尾插法建立单链表"><a href="#尾插法建立单链表" class="headerlink" title="尾插法建立单链表"></a>尾插法建立单链表</h5><p>尾插法从一个空表开始，生成新结点，读取数据存放到新结点的数据域中，然后将新结点插入到当前链表的表尾上，再用新结点来赋值给暂存结点，以达到每次暂存结点都是尾结点的目的。</p><h5 id="单链表结构与顺序存储结构优缺点"><a href="#单链表结构与顺序存储结构优缺点" class="headerlink" title="单链表结构与顺序存储结构优缺点"></a>单链表结构与顺序存储结构优缺点</h5><p>存储分配方式：</p><ul><li>顺序存储结构用一段连续的存储单元依次存储线性表的数据元素。</li><li>单链表采用链式存储结构，用一组任意的存储单元存放线性表的元素。<br>时间性能</li><li>查找<ul><li>顺序存储结构O(1)</li><li>单链表O(n)</li></ul></li><li>插入和删除<ul><li>顺序存储结构需要平均移动表长一半的元素，时间为O(n)</li><li>单链表再计算出某位置的指针后，插入和删除时间仅为O(1)</li></ul></li><li>空间性能<ul><li>顺序存储结构需要预分配存储空间，分大了，，容易造成空间的浪费，分小了，容易发生溢出。</li><li>单链表不需要分配存储空间，只要有就可以分配，元素个数也不受限制。</li></ul></li></ul><p>总之，线性表若需要频繁的查找，很少进行插入和删除操作时，宜采用顺序存储结构。如果需要频繁插入和删除时，宜采用单链表结构。</p><h5 id="静态链表"><a href="#静态链表" class="headerlink" title="静态链表"></a>静态链表</h5><p>用数组描述的链表叫做静态链表。</p><ul><li>我们对数组的第一个和最后一个元素做特殊处理，他们的data不存放数据。</li><li>我们通常把未使用的数组元素成为备用链表。</li><li>数组的第一个元素，即下标为0的那个元素的cur就存放备用链表的第一个结点的下标。</li><li>数组的最后一个元素，即下标为MAXSIZE-1的cur则存放第一个有数值的元素下标，相当于单链表中头结点的作用。<br>静态链表的插入，每当进行插入操作时，便可以从备用链表上取得第一个结点作为待插入的新结点。</li></ul><p>静态链表的删除。每当进行删除操作时，将指定元素上一位的下标改为下一位元素的下标，再将第一个元素的游标改为删除的下标。</p><h5 id="静态链表的优缺点"><a href="#静态链表的优缺点" class="headerlink" title="静态链表的优缺点"></a>静态链表的优缺点</h5><ul><li>优点<ul><li>在插入和删除操作时，只需要修改游标，不需要移动元素，从而改进了再顺序存储结构中的插入和删除操作需要移动大量元素的缺点。</li></ul></li><li>缺点  <ul><li>没有解决连续存储分配数组带来的表长难以确定的问题</li><li>失去了顺序存储结构随机存取的特性。<br>总之，静态链表其实时为了给没有指针的编程语言设计的一种实现单链表功能的方法。尽管我们可以用单链表就不用静态链表了，但这样的思考方式时非常巧妙的，应该理解其思想，以备不时之需。</li></ul></li></ul><h5 id="双链表"><a href="#双链表" class="headerlink" title="双链表"></a>双链表</h5><p>在线性表的链式存储结构中，每个物理节点增加一个指向后继节点的指针域和一个指向前驱节点的指针域。优点：从任一节点出发可以快速找到其前趋节点和后继节点，从任一节点出发可以访问其他节点。</p><pre><code class="hljs cpp"><span class="hljs-comment">//双链表的定义</span><span class="hljs-keyword">typedef</span> <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">DNode</span>&#123;</span>    ElemType data;    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">DNode</span> *<span class="hljs-title">prior</span>;</span> <span class="hljs-comment">//指向前趋节点</span>    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">DNode</span> *<span class="hljs-title">next</span>;</span> <span class="hljs-comment">//指向后继节点</span>&#125; DLinkList;</code></pre><h5 id="广义表"><a href="#广义表" class="headerlink" title="广义表"></a>广义表</h5><p>广义表是线性表的推广，对于线性表而言，n个元素都是基本的单元素。广义表中这些元素不仅可以使单元素也可以是另一个广义表。<br><pre><code class="hljs cpp"><span class="hljs-comment">//广义表的定义</span><span class="hljs-keyword">typedef</span> <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">GNode</span> *<span class="hljs-title">GList</span>;</span><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">GNode</span>&#123;</span>    <span class="hljs-keyword">int</span> Tag;    <span class="hljs-comment">//标志域：0表示结点是单元素，1表示结点是广义表。</span>    <span class="hljs-keyword">union</span>&#123;  <span class="hljs-comment">//子表指针区域Sublist与单元素数据域Data复用，即共用存储空间</span>        ElementType Data;        GList SubList;    &#125;URegion;    GList Next; <span class="hljs-comment">//指向后继结点</span>&#125;;</code></pre><br>多重链表：链表中的节点可能同时隶属于多个链。多重链表中节点的指针域会有多个，如上面的广义表包含了Next和SubList两个指针域。但是包含多个指针域的链表并不一定是多重链表，比如双向链表。另外树、图这样相对复杂的数据结构都可以采用多重链表的方式实现存储。</p><h4 id="栈"><a href="#栈" class="headerlink" title="栈"></a>栈</h4><p>栈是限定仅在表尾进行插入和删除操作的线性表。栈元素具有线性关系，即前驱后继关系。</p><h5 id="栈的结构定义"><a href="#栈的结构定义" class="headerlink" title="栈的结构定义"></a>栈的结构定义</h5><pre><code class="hljs cpp"><span class="hljs-keyword">typedef</span> <span class="hljs-keyword">int</span> SElemType   <span class="hljs-comment">//SElemType根据实际情况而定，这里假设为int。</span><span class="hljs-keyword">typedef</span> <span class="hljs-class"><span class="hljs-keyword">struct</span>&#123;</span>    SElemType data[MAXSIZE];    <span class="hljs-keyword">int</span> top;   <span class="hljs-comment">//用于栈顶指针</span>&#125;SqStack;</code></pre><p>堆栈：具有一定操作约束的线性表，旨在一端（栈顶，Top）做插入、删除操作。插入数据：入栈（Push）。删除数据出栈（Pop）。后入先出原则。</p><h5 id="入栈"><a href="#入栈" class="headerlink" title="入栈"></a>入栈</h5><pre><code class="hljs livescript"><span class="hljs-literal">void</span> Push(Stack Ptrs,ElementType item)&#123;    <span class="hljs-keyword">if</span>(Ptrs-&gt;Top == MaxSize-<span class="hljs-number">1</span>)&#123;        printf(<span class="hljs-string">&quot;堆栈满&quot;</span>):<span class="hljs-keyword">return</span>;    &#125;    <span class="hljs-keyword">else</span>&#123;        Ptrs-&gt;Data[++(Ptrs-&gt;Top)] = item;        <span class="hljs-keyword">return</span>;    &#125;&#125;</code></pre><h5 id="出栈"><a href="#出栈" class="headerlink" title="出栈"></a>出栈</h5><pre><code class="hljs coq">Sratus Pop(SqStack *S,SElemType *e)&#123;    <span class="hljs-keyword">if</span> (S-&gt;<span class="hljs-built_in">top</span>==<span class="hljs-number">-1</span>)        <span class="hljs-keyword">return</span> ERROE;    *e = S-&gt;data[S-&gt;<span class="hljs-built_in">top</span>];  //将要删除的栈顶元素赋值给*e    S-&gt;<span class="hljs-built_in">top</span>--;    //栈顶指针减<span class="hljs-number">1</span>    <span class="hljs-keyword">return</span> OK;&#125;</code></pre><p>两栈共享空间结构就是让栈底在数组的两端，这样两栈如果增加元素就是往中间靠拢。这样可以最大程度利用空间。</p><p>链栈是指栈的链式存储结构，栈顶放在单链表的头部。如果栈在使用过程中元素变化不可预料，那麽最好使用链栈，反之则使用顺序栈更好一些。</p><h6 id="递归"><a href="#递归" class="headerlink" title="递归"></a>递归</h6><p>我们把一个直接调用自己或经过一系列的调用语句间接地调用自己地函数称作递归函数。每个递归定义必须至少有一个条件，满足条件时递归不再进行，否则就会造成栈溢出。在递归前行阶段，对于每一层递归，函数的局部变量、参数值和返回地址被push。在退回阶段，位域栈顶的局部变量、参数值和返回地址被pop，用于返回调用层次中执行代码的其余部分也就是恢复了调用状态。</p><h5 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h5><p>队列是只允许在一段进行插入操作，而在在另一端进行删除操作的线性表。队列是一种先进先出的线性表，允许插入的一段称为队尾，允许删除的一端称为队头。</p><p>循环队列，循环队列是为了解决假溢出问题。我们把队列的这种头尾相接的顺序存储结构称为循环队列。</p>]]></content>
    
    
    <categories>
      
      <category>数据结构</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基本数据类型</title>
    <link href="/2020/09/26/C%E8%AF%AD%E8%A8%80%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
    <url>/2020/09/26/C%E8%AF%AD%E8%A8%80%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="1-C语言的基本数据类型"><a href="#1-C语言的基本数据类型" class="headerlink" title="1.C语言的基本数据类型"></a>1.C语言的基本数据类型</h2><h3 id="1-1变量"><a href="#1-1变量" class="headerlink" title="1.1变量"></a>1.1变量</h3><h5 id="1-1-1变量的解释"><a href="#1-1-1变量的解释" class="headerlink" title="1.1.1变量的解释"></a>1.1.1变量的解释</h5><p>变量类似一个盒子，你可以先把东西放里面（赋值），等用时再取出来。一个变量“盒子”理论上能放一个任何东西。但是我们需要贴上标签告诉别人这个盒子装的是什么东西（声明）。另外变量“盒子”有点特殊，他只能放入一个东西。如果一个“盒子”里已经有一个东西，但是你又放入了一个东西，原本东西就会被挤出去。<br>所以在c语言中变量的声明和赋值有两种方法：</p><ol><li>声明并赋值</li><li>先声明后赋值</li></ol><h5 id="1-1-2变量命名规则"><a href="#1-1-2变量命名规则" class="headerlink" title="1.1.2变量命名规则"></a>1.1.2变量命名规则</h5><p>c语言中变量名是不能随便起的，他也要遵循一定的规则。</p><ol><li>变量名中只能出现英文，数字，下划线和美元符号。</li><li>变量名不能以数字开头</li><li>大小写敏感。A和a是两个变量。</li><li>变量名不能和关键字相同。</li></ol><h6 id="1-1-3拓展"><a href="#1-1-3拓展" class="headerlink" title="1.1.3拓展"></a>1.1.3拓展</h6><p>c语言中的32个关键字。</p><ul><li>auto、double、int、struct；</li><li></li><li>break、else、long、switch；</li><li></li><li>case、enum、register、typedef；</li><li></li><li>char、extern、return、union；</li><li></li><li>const、float、short、unsigned；</li><li></li><li>continue、for、signed、void；</li><li></li><li>default、goto、sizeof、volatile；</li><li></li><li>do、if、while、static；</li></ul><h3 id="1-2基本数据类型"><a href="#1-2基本数据类型" class="headerlink" title="1.2基本数据类型"></a>1.2基本数据类型</h3><p>编译器要处理数据，就需要知道数据的存储大小和存储方式，即数据类型。<br>常用的基本数据类型有</p><ul><li>字符char，字符型在本质上就是整型，在c语言中用char表示一个字符型。他表示占用一个字符的存储空间。当我们读出时将会得到一个整形数据，而我们输出时会得到一个字符是因为我们人为的定义了一个对照表，这个表规定字符a的数值就是97，所以当我们遇到97时我们有两种读出方式，第一种以整数形式读出就是97，另一种就是以字符型读出，使用%c指定读出形式，则对照表则为a。</li><li>整数int，整数又称作整数型，就是我们平时看到的准确的整数，一般占4个字节（32位）。最高位代表符号，0表示正数，1表示负数，取值范围是-2147483648~2147483647。</li><li>浮点型，浮点型又称实型。<ul><li>单精度浮点数float，系统中基本的浮点类型。我们在c语言中使用float表示一个单精度浮点类型。一个float类型占用4个字节储存位。使用%f作为其格式说明符，而且float一般精确到小数点后6位。</li><li>双精度浮点数double，范围更大的浮点类型。比float类型表示更多的有效数字以及更大的指数。我们在c语言中使用一个double类型表示一个双精度的浮点类。一个double类型占用8个字节，其内存消耗是double的两倍，double运算速度比float慢得多。double的格式说明符仍然是%f，而且double类型一般精确到小数位后15位。long double范围更大的浮点类型。我们使用%lf作为其格式说明符。<br>基本上c语言最常用的就是这四个数据类型。</li></ul></li></ul><h3 id="1-3数据类型汇总表格"><a href="#1-3数据类型汇总表格" class="headerlink" title="1.3数据类型汇总表格"></a>1.3数据类型汇总表格</h3><div class="table-container"><table><thead><tr><th>名称</th><th>全称类型说明符</th><th>缩写类型说明符</th><th>位数</th><th>范围</th></tr></thead><tbody><tr><td>整型</td><td>int</td><td>int</td><td>16位</td><td>-32768至+32767</td></tr><tr><td>无符号整型</td><td>unsigned int</td><td>unsigned</td><td>16位</td><td>0 至 65535</td></tr><tr><td>短整型</td><td>short int</td><td>short</td><td>16位</td><td>-32768至+32767</td></tr><tr><td>无符号短整型</td><td>unsigned short int</td><td>unsigned short</td><td>16位</td><td>0 至 65535</td></tr><tr><td>长整型</td><td>long int</td><td>long</td><td>32位</td><td>-2^63 ~ 2^63-1</td></tr><tr><td>无符号长整型</td><td>unsigned long int</td><td>unsigned long</td><td>32位</td><td>0至4,294,967,295</td></tr><tr><td>单精度浮点型</td><td>float</td><td>float</td><td>32位</td><td>-3.4<em>10(-38)至3.4</em>10(38)</td></tr><tr><td>双精度浮点型</td><td>double</td><td>double</td><td>64位</td><td>-16 -1.7<em>10(-308)～1.7</em>10(308)</td></tr><tr><td>长双精度浮点型</td><td>long double</td><td>long double</td><td>128位</td><td>-1.2<em>10(-4932)～1.2</em>10(4932)</td></tr></tbody></table></div><h3 id="1-4基本类型的注意事项"><a href="#1-4基本类型的注意事项" class="headerlink" title="1.4基本类型的注意事项"></a>1.4基本类型的注意事项</h3><h5 id="1-4-1表示其他进制"><a href="#1-4-1表示其他进制" class="headerlink" title="1.4.1表示其他进制"></a>1.4.1表示其他进制</h5><p>只有整数可以表示其他进制。</p><ol><li>十六进制。以0x或0X开头（0是阿拉伯数字0）</li><li>八进制。以0开头（0是阿拉伯数字0）</li><li>二进制。以0b或0B开头（0是阿拉伯数字0）</li></ol><h5 id="1-4-2-科学计数法"><a href="#1-4-2-科学计数法" class="headerlink" title="1.4.2 科学计数法"></a>1.4.2 科学计数法</h5><p>只有double类型可以进行科学计数法。</p><pre><code class="hljs apache"><span class="hljs-attribute">double</span> a = <span class="hljs-number">3</span>E<span class="hljs-number">2</span>; //表示<span class="hljs-number">3</span> x <span class="hljs-number">10</span>^<span class="hljs-number">2</span>，所以a = <span class="hljs-number">300</span></code></pre><h5 id="1-4-3-长整型long的问题"><a href="#1-4-3-长整型long的问题" class="headerlink" title="1.4.3 长整型long的问题"></a>1.4.3 长整型long的问题</h5><p>声明长整型时数的后面要加上L</p><pre><code class="hljs livecodeserver"><span class="hljs-keyword">long</span> <span class="hljs-keyword">a</span> = <span class="hljs-number">666</span>L; <span class="hljs-comment"> //L大小写都可以，但为了便于区分，一般使用大写L</span></code></pre><h5 id="1-4-4-浮点型float问题"><a href="#1-4-4-浮点型float问题" class="headerlink" title="1.4.4 浮点型float问题"></a>1.4.4 浮点型float问题</h5><p>声明float时数的后面要加上F，因为默认的浮点型都是double</p><pre><code class="hljs angelscript"><span class="hljs-built_in">float</span> a = <span class="hljs-number">3.14</span>F;    <span class="hljs-comment">//F大小写都可以</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>C语言</category>
      
    </categories>
    
    
    <tags>
      
      <tag>C语言</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据可视化总结</title>
    <link href="/2020/09/25/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <url>/2020/09/25/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<p>数据可视化指的是通过可视化表示来探索数据，它与数据挖掘紧密相关。</p><p>总的来说数据可视化能让人们更直观的看到数据的特点。其实数据可视化也是对数据进行处理，只不过是以图表方式来表现的。</p><p>下面将介绍几种用matplotlib库来实现可视化的方法。<br>数据可视化还有数据描述性统计，就是算出方差，均值，中位数等等。</p><pre><code class="hljs sas">def des(self,n):    def status(<span class="hljs-meta">x</span>):        <span class="hljs-meta">return</span> pd.Series([<span class="hljs-meta">x</span>.count(),<span class="hljs-meta">x</span><span class="hljs-meta">.min(</span>),<span class="hljs-meta">x</span>.id<span class="hljs-meta">xmin(</span>),<span class="hljs-meta">x</span>.quantile(.25),<span class="hljs-meta">x</span>.medi<span class="hljs-meta">an(</span>),              <span class="hljs-meta">x</span>.quantile(.75),<span class="hljs-meta">x</span><span class="hljs-meta">.mean(</span>),<span class="hljs-meta">x</span><span class="hljs-meta">.max(</span>),<span class="hljs-meta">x</span>.id<span class="hljs-meta">xmax(</span>),<span class="hljs-meta">x</span>.mad(),<span class="hljs-meta">x</span><span class="hljs-meta">.var(</span>),              <span class="hljs-meta">x</span><span class="hljs-meta">.std(</span>),<span class="hljs-meta">x</span>.skew(),<span class="hljs-meta">x</span>.kurt()],<span class="hljs-meta">index</span>=[<span class="hljs-string">&#x27;总数&#x27;</span>,<span class="hljs-string">&#x27;最小值&#x27;</span>,<span class="hljs-string">&#x27;最小值位置&#x27;</span>,<span class="hljs-string">&#x27;25%分位数&#x27;</span>,            <span class="hljs-string">&#x27;中位数&#x27;</span>,<span class="hljs-string">&#x27;75%分位数&#x27;</span>,<span class="hljs-string">&#x27;均值&#x27;</span>,<span class="hljs-string">&#x27;最大值&#x27;</span>,<span class="hljs-string">&#x27;最大值位数&#x27;</span>,<span class="hljs-string">&#x27;平均绝对偏差&#x27;</span>,<span class="hljs-string">&#x27;方差&#x27;</span>,<span class="hljs-string">&#x27;标准差&#x27;</span>,<span class="hljs-string">&#x27;偏度&#x27;</span>,<span class="hljs-string">&#x27;峰度&#x27;</span>])df = pd.DataFrame(n, columns=[<span class="hljs-string">&#x27;x1&#x27;</span>,<span class="hljs-string">&#x27;x2&#x27;</span>,<span class="hljs-string">&#x27;x3&#x27;</span>])#输入数组p<span class="hljs-meta">rint(</span>df.head())p<span class="hljs-meta">rint(</span>df.apply(status))</code></pre><p>一、直方图</p><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> matplotlib <span class="hljs-keyword">as</span> plt<span class="hljs-title">plt</span>.hist(<span class="hljs-class"><span class="hljs-keyword">data</span>, normed=1)</span><span class="hljs-meta">#其实hist函数还有很多参数</span></code></pre><p>二、水平条形图</p><pre><code class="hljs routeros"><span class="hljs-comment"># 指定默认字体（防止中文出现乱码）</span>mpl.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;FangSong&#x27;</span>]  # 指定‘仿宋’字体<span class="hljs-comment"># 水平柱形图---条形图</span>fig, ax = plt.subplots()ax.barh(x, y, 0.5, <span class="hljs-attribute">color</span>=<span class="hljs-string">&#x27;skyblue&#x27;</span>)  # 0.5是设置的柱子的宽度ax.<span class="hljs-builtin-name">set</span>(<span class="hljs-attribute">title</span>=<span class="hljs-string">&#x27;水平条形图&#x27;</span>, <span class="hljs-attribute">xlabel</span>=<span class="hljs-string">&#x27;x&#x27;</span>, <span class="hljs-attribute">ylabel</span>=<span class="hljs-string">&#x27;y&#x27;</span>)plt.show()</code></pre><p>三、热力图</p><pre><code class="hljs routeros"><span class="hljs-comment"># 绘制x-y-z的热力图，比如 年-月-销量 的热力图</span>f, ax = plt.subplots(figsize=(9, 6))<span class="hljs-comment">#绘制热力图，还要将数值写到热力图上</span><span class="hljs-comment">#每个网格上用线隔开,data是df类型</span>sns.heatmap(data, <span class="hljs-attribute">annot</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">fmt</span>=<span class="hljs-string">&quot;d&quot;</span>, <span class="hljs-attribute">linewidths</span>=0.5, <span class="hljs-attribute">ax</span>=ax,cmap = <span class="hljs-string">&quot;YlGnBu&quot;</span>)<span class="hljs-comment">#设置坐标字体方向并保持水平对齐</span>label_y = ax.get_yticklabels()plt.setp(label_y, <span class="hljs-attribute">rotation</span>=360, <span class="hljs-attribute">horizontalalignment</span>=<span class="hljs-string">&#x27;right&#x27;</span>)label_x = ax.get_xticklabels()plt.setp(label_x, <span class="hljs-attribute">rotation</span>=45, <span class="hljs-attribute">horizontalalignment</span>=<span class="hljs-string">&#x27;right&#x27;</span>)plt.show()</code></pre><p>四、折线图<br><pre><code class="hljs routeros"><span class="hljs-comment"># 获取&#x27;column&#x27;这一列的值</span>y = df[<span class="hljs-string">&#x27;column&#x27;</span>].values<span class="hljs-comment"># 获取索引列的值（年份）</span>x = df.index.values<span class="hljs-comment"># 指定默认字体（防止中文出现乱码）</span>mpl.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;FangSong&#x27;</span>]  # 指定‘仿宋’字体<span class="hljs-comment"># 折线图</span>fig, ax = plt.subplots()ax.plot(x, y)ax.<span class="hljs-builtin-name">set</span>(<span class="hljs-attribute">title</span>=None, <span class="hljs-attribute">xlabel</span>=None, <span class="hljs-attribute">ylabel</span>=None)  # 中文出现乱码，要先设置字体plt.show()</code></pre></p><p>五、散点图</p><pre><code class="hljs routeros"><span class="hljs-comment"># 指定默认字体（防止中文出现乱码）</span>mpl.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;FangSong&#x27;</span>]  # 指定‘仿宋’字体<span class="hljs-comment"># 折线图</span>fig, ax = plt.subplots()ax.scatter(x, y)ax.<span class="hljs-builtin-name">set</span>(<span class="hljs-attribute">title</span>=None, <span class="hljs-attribute">xlabel</span>=None, <span class="hljs-attribute">ylabel</span>=None)  # 中文出现乱码，要先设置字体plt.show()</code></pre>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征工程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据预处理方法总结</title>
    <link href="/2020/09/25/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"/>
    <url>/2020/09/25/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/</url>
    
    <content type="html"><![CDATA[<p>  数据是机器学习的原料，机器学习是通过对数据的训练才得到某种特性的。因此正确的预处理数据对模型结果的输出尤为重要。</p><blockquote><p>-一、数据可能存在的问题<br>  数据一开始可能有数据重复，数据缺失，数据存在异常值等情况。<br> 二、数据预处理的步骤<br>数据处理主要包括数据的清洗，数据的转化.数据描述，特征选择和特征抽取这几个步骤。</p><p>1、数据清洗主要通过对缺失值，异常值和重复数据的处理来完成的。</p></blockquote><p>对缺失值的处理首先要判断是否有缺失值，用isnull函数，这个函数返回的是一个布尔类型的结果。其中缺失值为True，非缺失值为Flase。<br>然后再进行缺失值的处理.</p><hr><p>缺失值处理主要有以下两种方式。<br>一是删除缺失记录<br>删除数据通过pandas的dropna函数来删除缺失值。</p><pre><code class="hljs angelscript">df.dropna(axis = <span class="hljs-number">0</span>，thresh = <span class="hljs-number">2</span>)</code></pre><p>axis = 0代表沿着竖直方向（删除行），1代表沿着水平方向（删除列）。使用dropna函数可以直接删除缺失数据所在行或列（默认是行）</p><p>thresh  = 2代表保留至少2个非NaN数据所在的行。默认是不保留</p><hr><blockquote><p>二是填充数据<br>填充数据用到的pandas里的函数是</p></blockquote><pre><code class="hljs python">fillna(value=<span class="hljs-literal">None</span>, method=<span class="hljs-literal">None</span>, axis=<span class="hljs-literal">None</span>, inplace=<span class="hljs-literal">False</span>, limit=<span class="hljs-literal">None</span>, downcast=<span class="hljs-literal">None</span>, **kwargs)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">vlaue参数指的是填充的值</span><span class="hljs-string">method指填充的方法pad/ffill表示用之前的数据填充（列，第一行不填充）backfill/bfill表示用之后的数据填充（列。最后一行不填充）。</span><span class="hljs-string">limit指限制填充的次数</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre><p>。<br>另外还有平均值填补，拉格朗日插值法。平均值就是用mean()函数求出平均值。</p><p>拉格朗日插值法是通过其他存在值的计算，来拟合出缺失的数据，不过在缺失大量数据时，偏差较大。</p><blockquote><p>异常值的处理也主要有两种方法。<br>一是利用3σ原则来找出异常值。就是如果数据服从正态分布，异常值则为与平均值相差超过3倍σ的值。</p></blockquote><pre><code class="hljs subunit">u = data.mean()#计算均值std = data.std()#计算标准差<span class="hljs-keyword">error </span>= data[np.bas(data-u)&gt;3*std]#求出异常值</code></pre><blockquote><p>二是箱型图分析</p></blockquote><p>箱线图是一种用于显示一组数据分散情况的统计图。箱型图不受异常值影响，可以以一种相对稳定的方式描述数据的离散分布情况。</p><p>下面是python代码实现方法。其中的precentile函数是用来求出数组的上四分位，中位和下四分位数的。</p><p>percentile = np.percentile(self.n, (25, 50, 75), interpolation=’midpoint’)</p><h1 id="以下为箱线图的五个特征值"><a href="#以下为箱线图的五个特征值" class="headerlink" title="以下为箱线图的五个特征值"></a>以下为箱线图的五个特征值</h1><pre><code class="hljs ini"><span class="hljs-attr">Q1</span> = percentile[<span class="hljs-number">0</span>]<span class="hljs-comment">#上四分位数</span><span class="hljs-attr">Q3</span> = percentile[<span class="hljs-number">2</span>]<span class="hljs-comment">#下四分位数</span><span class="hljs-attr">IQR</span> = Q3 - Q1<span class="hljs-comment">#四分位距</span><span class="hljs-attr">ulim</span> = Q3 + <span class="hljs-number">1.5</span>*IQR<span class="hljs-comment">#上限 非异常范围内的最大值</span><span class="hljs-attr">llim</span> = Q1 - <span class="hljs-number">1.5</span>*IQR<span class="hljs-comment">#下限 非异常范围内的最小值</span></code></pre><p>2、在数据的转换阶段，有对数据进行归一化，标准化，离散化等操作。</p><p>首先，归一化操作是将数据统一映射到[0,1]区间上。由于数据相差过大，所以一般要做归一化处理。</p><pre><code class="hljs gml">range_ = np.<span class="hljs-built_in">max</span>(<span class="hljs-symbol">x</span>) - np.<span class="hljs-built_in">min</span>(<span class="hljs-symbol">x</span>)num = (<span class="hljs-symbol">x</span>-np.<span class="hljs-built_in">min</span>(<span class="hljs-symbol">x</span>)) / range_</code></pre><p>标准化数据，保证每个维度的特征数据方差为1，均值为0。使得预测结果不会被某些维度过大的特征值而主导。</p><pre><code class="hljs ini"><span class="hljs-attr">mu</span> = np.mean(self.x,axis=<span class="hljs-number">0</span>)<span class="hljs-comment">#计算平均数</span><span class="hljs-attr">sigma</span> = np.std(self.x,axis=<span class="hljs-number">0</span>)<span class="hljs-comment">#计算方差</span><span class="hljs-attr">num</span> =  (x-mu) / sigma</code></pre><p>连续值离散化是将数据按照不同的区间分好类，有等宽法和等频法。</p><p>等宽法是将数据平均分成三个相同的区间。</p><pre><code class="hljs vala">d1 = pd.cut(df,k, labels = range(k))<span class="hljs-meta">#k表示划分区间的个数</span><span class="hljs-meta">#df表示输入的dataframe类型的数据</span><span class="hljs-meta">#lables表示表头</span></code></pre><p>等频法是按数据出现频率划分。将相同数量的记录放在每个区间，保证每个区间的数量基本一致。即分组后，每个分组的元素个数是一样的。</p><pre><code class="hljs apache"><span class="hljs-attribute">k</span> = <span class="hljs-number">4</span><span class="hljs-attribute">w</span> =<span class="hljs-meta"> [1.0*i/k for i in range(k+1)]</span><span class="hljs-attribute">w</span> = data.describe(percentiles = w)[<span class="hljs-number">4</span>:<span class="hljs-number">4</span>+k+<span class="hljs-number">1</span>]#取几个分位数的值作为不等长列表，用于cut函数<span class="hljs-attribute">w</span>[<span class="hljs-number">0</span>] = w[<span class="hljs-number">0</span>]*(<span class="hljs-number">1</span>-<span class="hljs-number">1</span>e-<span class="hljs-number">10</span>)#浮点化<span class="hljs-attribute">d2</span> = pd.cut(data, w, labels = range(k))<span class="hljs-comment">#或者直接用qcut(data，k)函数</span></code></pre><p>还有另外一种数据处理操作是离散值处理（独热编码）<br>独热编码是将同类型的数据分类按照二进制来变成0或1的数字。例如：<br>男：01，女：10</p><p>学生：001，老师：010，领导：100</p><pre><code class="hljs ini"><span class="hljs-attr">date</span> = pd.read_csv(self.n)<span class="hljs-comment">#读取csv文件</span><span class="hljs-attr">date_df</span> = pd.DataFrame(date)<span class="hljs-comment">#转换成DataFrame格式</span><span class="hljs-attr">date_gd</span> = pd.get_dummies(date_df)<span class="hljs-comment">#进行编码</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征工程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>概率论（下）</title>
    <link href="/2020/09/25/%E6%A6%82%E7%8E%87%E8%AE%BA%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <url>/2020/09/25/%E6%A6%82%E7%8E%87%E8%AE%BA%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h3 id="二维随机变量"><a href="#二维随机变量" class="headerlink" title="二维随机变量"></a>二维随机变量</h3><h5 id="二维随机变量概念"><a href="#二维随机变量概念" class="headerlink" title="二维随机变量概念"></a>二维随机变量概念</h5><p>在同一随机试验E中，称定义在同一样本空间上的两个随机变量X,Y构成的有序数组(X,Y)为二维随机变量。对每个样本点w，有两个实数X(w),Y(w)与之对应且满足：</p><ul><li>对任意实数x，事件{w|X(w)&lt;=x}都有确定的概率</li><li>对任意实数y，事件{w|Y(w)&lt;=y}都有确定的概率<h5 id="n维随机变量"><a href="#n维随机变量" class="headerlink" title="n维随机变量"></a>n维随机变量</h5>设随机试验E的样本空间为Ω，X1,X2,…,Xn为定义在Ω上的n个随机变量，称它们构成的有序数组（X1,X2,…,Xn）为n维随机变量。<h5 id="二维随机变量积其分布"><a href="#二维随机变量积其分布" class="headerlink" title="二维随机变量积其分布"></a>二维随机变量积其分布</h5>定义：对任意实数（X,Y）∈R^2记</li></ul><script type="math/tex; mode=display">\{X \leq x, Y \leq y\}=\{X \leq x\} \cap\{Y \leq y\}</script><p>称二元函数 </p><script type="math/tex; mode=display">F(x, y)=P\{X \leq x, Y \leq y\}  为  (X, Y)</script><p>的联合分布函数。 一维随机变量 X、Y的分布函数</p><script type="math/tex; mode=display">F  _{X}(x)  与  F_{Y}(y)</script><p>  称为  (X, Y)  的边缘分布函数。</p><p>由联合分布函数可确定边缘分布函数</p><script type="math/tex; mode=display">\begin{array}{l}F_{X}(x)=P\{X \leq x\}=P\{X \leq x, Y<+\infty\}=\lim _{y \rightarrow+\infty} F(x, y) \\F_{Y}(y)=P\{Y \leq y\}=P\{X<+\infty, Y \leq y\}=\lim F(x, y)\end{array}\begin{array}{l}P\left\{x_{1}<X \leq x_{2}, y_{1}<Y \leq y_{2}\right\} \\=F\left(x_{2}, y_{2}\right)-F\left(x_{1}, y_{2}\right) \\\quad-F\left(x_{2}, y_{1}\right)+F\left(x_{1}, y_{1}\right)\end{array}`</script><h5 id="联合分布函数的性质"><a href="#联合分布函数的性质" class="headerlink" title="联合分布函数的性质"></a>联合分布函数的性质</h5><ol><li><p>单调不减性：F(x,y)分别对x，y单调不减。</p><script type="math/tex; mode=display">\begin{array}{ll}x_{1}<x_{2} & F\left(x_{1}, y\right) \leq F\left(x_{2}, y\right) \\y_{1}<y_{2} & F\left(x, y_{1}\right) \leq F\left(x, y_{2}\right)\end{array}</script></li><li><p>有界性：0&lt;=F(x，y)&lt;=1</p></li></ol><script type="math/tex; mode=display">\begin{array}{l}\lim _{x \rightarrow-\infty} F(x, y)=0 \\\lim _{y \rightarrow-\infty} F(x, y)=0 \quad \lim _{x \rightarrow+\infty \atop y \rightarrow+\infty} F(x, y)=1\end{array}</script><p>3.右连续性：F(x，y)分别关于x或y为右连续。</p><script type="math/tex; mode=display">\lim _{x \rightarrow x_{0}^{+}} F(x, y)=F\left(x_{0}, y\right) \quad \lim _{y \rightarrow y_{0}^{+}} F(x, y)=F\left(x, y_{0}\right)</script><p>4.相容性：对任意x1&lt;x2,y1&lt;y2,有：</p><script type="math/tex; mode=display">F\left(x_{2}, y_{2}\right)-F\left(x_{1}, y_{2}\right)-F\left(x_{2}, y_{1}\right)+F\left(x_{1}, y_{1}\right) \geq 0</script><p>如果二元函数F(x，y)满足上述4个性质，则必存在二维随机变量(X,Y)以F(x,y)为分布函数。</p><h5 id="n维随机变量-1"><a href="#n维随机变量-1" class="headerlink" title="n维随机变量"></a>n维随机变量</h5><p>定义：n维随机变量</p><script type="math/tex; mode=display">(  \left.X_{1}, X_{2}, \ldots, X_{\mathrm{n}}\right)</script><p>的联合分布函数：</p><script type="math/tex; mode=display">F\left(x_{1}, x_{2}, \ldots x_{n}\right)=P\left\{X_{1} \leq x_{1}, X_{2} \leq x_{2}, \ldots X_{n} \leq x_{n}\right\}</script><p>式子中 </p><script type="math/tex; mode=display">x_{1}, x_{2}, \ldots, x_{\mathrm{n}}</script><p> 为n个任意实数。<br>由</p><script type="math/tex; mode=display">(  \left.X_{1}, X_{2}, \ldots, X_{\mathrm{n}}\right)</script><p> 的联合分布函数可确定。其中任意k个分量的联合分布函数，称为k维边缘分布函数.例如：  </p><script type="math/tex; mode=display">\quad F_{X_{1}}\left(x_{1}\right)=F\left(x_{1},+\infty,+\infty, \cdots,+\infty\right) F_{X_{1}, X_{2}}\left(x_{1}, x_{2}\right)=F\left(x_{1}, x_{2},+\infty, \cdots,+\infty\right)</script><h5 id="二维离散型随机变量及其联合分布律的定义"><a href="#二维离散型随机变量及其联合分布律的定义" class="headerlink" title="二维离散型随机变量及其联合分布律的定义"></a>二维离散型随机变量及其联合分布律的定义</h5><p> 定义：设二维随机变量(X,Y)至多取可列对数值: </p><script type="math/tex; mode=display">\left(x_{i}, y_{j}\right), i, j=1,2, \cdots . .</script><p>记</p><script type="math/tex; mode=display">P\left\{X=x_{i}, Y=y_{j}\right\}=p_{i j} \quad i, j=1,2, \ldots</script><p>若</p><script type="math/tex; mode=display">1)  \quad p_{i j} \geq 0 ; \quad i, j=1,2, \ldots \\2)  \sum_{i=1}^{\infty} \sum_{j=1}^{\infty} p_{i j}=1</script><p>称(X, Y)为二维离散型随机变量，称上式为(X, Y)的联合分布律.</p><h5 id="二维离散型随机变量的性质"><a href="#二维离散型随机变量的性质" class="headerlink" title="二维离散型随机变量的性质"></a>二维离散型随机变量的性质</h5><p>1.联合分布函数为</p><script type="math/tex; mode=display">F(x, y)=P\{X \leq x, Y \leq y\}=\sum_{x_{i} \leq x y_{j} \leq y} p_{i j}</script><p>2.随机变量X和Y的分布律为</p><script type="math/tex; mode=display">P\left\{X=x_{i}\right\}=p_{i}=\sum_{j=1}^{\infty} p_{i j} \quad(i=1,2, \cdots) \quad \\ P\left\{Y=y_{i}\right\}=p_{\cdot j}=\sum_{i=1}^{\infty} p_{i j} \quad(j=1,2, \cdots)</script><h5 id="二维连续型随机变量及其联合概率密度的定义"><a href="#二维连续型随机变量及其联合概率密度的定义" class="headerlink" title="二维连续型随机变量及其联合概率密度的定义"></a>二维连续型随机变量及其联合概率密度的定义</h5><p>设二维随机变量(X,Y)的联合分布函数为F(x,y),如果存在非负的函数f(x,y)使得对任意实数对(x,y)∈R^2,有</p><script type="math/tex; mode=display">F(x, y)=\int_{-\infty}^{y} \int_{-\infty}^{x} f(u, v) d u d v</script><p>称(X,Y)为连续型随机变量，称f(x,y)为(X,Y)的联合概率密度。</p><h5 id="联合概率密度的性质"><a href="#联合概率密度的性质" class="headerlink" title="联合概率密度的性质"></a>联合概率密度的性质</h5><p>1.</p><script type="math/tex; mode=display">1)  f(x, y) \geq 0 \\2)  \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} f(x, y) d x d y=1</script><p>2.如果联合概率密度函数f(x,y)在(x,y)处连续，则有</p><script type="math/tex; mode=display">\frac{\partial^{2} F(x, y)}{\partial x \partial y}=f(x, y)</script><p>3.若区域G∈R^2，则有</p><script type="math/tex; mode=display">P\{(X, Y) \in G\}=\iint_{G} f(x, y) d x d y</script><h5 id="二维随机变量的联合分布函数与边缘分布函数的关系"><a href="#二维随机变量的联合分布函数与边缘分布函数的关系" class="headerlink" title="二维随机变量的联合分布函数与边缘分布函数的关系"></a>二维随机变量的联合分布函数与边缘分布函数的关系</h5><p>设二维随机变量( X，Y)的联合分布函数为F(x, y)，则X、Y都是随机变量，且X、Y 的分布函数分别为</p><script type="math/tex; mode=display">\begin{array}{l}F_{X}(x)=\lim _{y \rightarrow+\infty} F(x, y)=F(x,+\infty) \\F_{Y}(y)=\lim _{x \rightarrow+\infty} F(x, y)=F(+\infty, y)\end{array}</script><p>此时称F(x)，F(y)为(X,Y)关于X、Y的边缘分布函数</p><hr><p>设二维离散型随机变量(X,Y)的联合分布律为  </p><script type="math/tex; mode=display">P\left\{X=x_{i}, Y=y_{j}\right\}=p_{i j} ，i、j=1,2,...</script><p>则X、Y都是离散型随机变量，且X、Y 的分布律分别为</p><script type="math/tex; mode=display">\begin{array}{l}P\left\{X=x_{i}\right\}=p_{i}=\sum_{j=1}^{\infty} p_{i j} \quad(i=1,2, \cdots) \\P\left\{Y=y_{i}\right\}=p_{. j}=\sum_{i=1}^{\infty} p_{i j} \quad(j=1,2, \cdots)\end{array}</script><p>此时称pi、pj 为(X，Y) 关于X、Y的边缘分布律.</p><hr><p>设二维连续型随机变量(  X, Y)  的联合概率密度为  f(x, y), \quad(x, y) \in R^{2}<br>则X、Y都是连续型随机变量，且X、Y 的概率密度分别为</p><script type="math/tex; mode=display">f_{X}(x)=\int_{-\infty}^{+\infty} f(x, y) d y, \quad f_{Y}(y)=\int_{-\infty}^{+\infty} f(x, y) d x</script><p>此时称 </p><script type="math/tex; mode=display">\left.f_{X}(x) 、 f_{Y}(y) \text { 为( } X, Y\right)</script><p> 关于X、Y 的边缘概率密度.</p><h5 id="随机变量的独立性"><a href="#随机变量的独立性" class="headerlink" title="随机变量的独立性"></a>随机变量的独立性</h5><p>设(X,Y)是二维随机变量，若对任意实数对(x,y)均有</p><script type="math/tex; mode=display">\boldsymbol{P}\{\boldsymbol{X} \leq \boldsymbol{x}, \boldsymbol{Y} \leq \boldsymbol{y}\}=\boldsymbol{P}\{\boldsymbol{X} \leq \boldsymbol{x}\} \boldsymbol{P}\{\boldsymbol{Y} \leq \boldsymbol{y}\}</script><p>成立，称X与Y相互独立，否则称它们是相依的。</p><p>注意：只要有一个实数对(x0,y0),使定义中的式子不成立，则X和Y不相互独立。</p><p>等价条件：<br>1.分布函数形式</p><script type="math/tex; mode=display">F(x, y)=F_{X}(x) F_{Y}(y)</script><p>2.分布律形式</p><script type="math/tex; mode=display">\boldsymbol{P}\left\{\boldsymbol{X}=\boldsymbol{x}_{i}, \boldsymbol{Y}=\boldsymbol{y}_{j}\right\}=\boldsymbol{P}\left\{\boldsymbol{X}=\boldsymbol{x}_{i}\right\} \boldsymbol{P}\left\{\boldsymbol{Y}=\boldsymbol{y}_{j}\right\}</script><p>3.概率密度形式</p><script type="math/tex; mode=display">f(x, y)=f_{X}(x) f_{Y}(y)</script><h5 id="多维随机变量独立性"><a href="#多维随机变量独立性" class="headerlink" title="多维随机变量独立性"></a>多维随机变量独立性</h5><p>设n维随机变量(X1,X2,…,Xn)的联合分布函数为F(x1,x2,…,xn),若对任意实数x1,x2,…,xn均有</p><script type="math/tex; mode=display">F\left(x_{1}, x_{2}, \cdots, x_{n}\right)=\prod_{k=1}^{n} F_{x_{k}}\left(x_{k}\right)</script><p>则称(X1,X2…Xn)相互独立。</p><p>随机变量的相互独立实质上是随机事件的相互独立。</p><h6 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h6><p>若n维随机变量(X1,X2,…,Xn)相互独立，则</p><ol><li>其中任意k个随机变量也相互独立<br>2.(X1,X2,…,Xn)两两独立。（两两独立不一定相互独立）<br>3.m维随机向量与n维随机向量相互独立。（两个不相交的子向量也相互独立）<br>4.函数随机向量也相互独立<h5 id="离散型条件分布"><a href="#离散型条件分布" class="headerlink" title="离散型条件分布"></a>离散型条件分布</h5>设(X,Y)的联合分布律为:<script type="math/tex; mode=display">P\left\{X=x_{i}, Y=y_{j}\right\}=p_{i j} \quad i, j=1,2, \cdots</script>若<script type="math/tex; mode=display">P\left\{Y=y_{j}\right\rangle>0</script>,则在事件<script type="math/tex; mode=display">\left\{Y=y_{j}\right\}</script>发生的条件下，事件<script type="math/tex; mode=display">\left\{X=x_{i}\right\} i= 1,2,\cdots</script>发生的条件概率为<script type="math/tex; mode=display">P\left\{X=x_{i} \mid Y=y_{j}\right\}=\frac{p_{i j}}{p_{. j}} \quad i=1,2, \cdots</script>则此概率数列具有分布律的性质</li><li>P{X=xi|Y=yi}&gt;=0 i=1,2,…<br>2.<script type="math/tex; mode=display">\sum_{i=1}^{+\infty} P\left\{X=x_{i} \mid Y=y_{j}\right\}=1</script></li></ol><p>则称上述分布律为在Y=yj的条件下，随机变量X的分布律。</p><h6 id="如何判断两个离散型随机变量X，Y相互独立。"><a href="#如何判断两个离散型随机变量X，Y相互独立。" class="headerlink" title="如何判断两个离散型随机变量X，Y相互独立。"></a>如何判断两个离散型随机变量X，Y相互独立。</h6><script type="math/tex; mode=display">1)  F(x, y)=F_{X}(x) F_{Y}(y) \\2)  P_{i j}=P_{i} P_{\cdot j}\\3)  P\left\{X=x_{i}\right\}=P\left\{X=x_{i} \mid Y=y_{j}\right\} \\4)  P\left\{Y=y_{j}\right\}=P\left\{Y=y_{j} \mid X=x_{i}\right\} \quad(i, j=1,2, \cdots)</script><h5 id="离散型随机变量的条件分布函数"><a href="#离散型随机变量的条件分布函数" class="headerlink" title="离散型随机变量的条件分布函数"></a>离散型随机变量的条件分布函数</h5><script type="math/tex; mode=display">F_{X \mid Y}(x \mid y)=P\left\{X \leq x \mid Y=y_{j}\right\}=\sum_{x_{i} \leq x} P\left\{X=x_{i} \mid Y=y_{j}\right\}=\sum_{x_{i} \leq x} \frac{p_{i j}}{p_{j}}</script><h5 id="连续型条件分布函数"><a href="#连续型条件分布函数" class="headerlink" title="连续型条件分布函数"></a>连续型条件分布函数</h5><script type="math/tex; mode=display">定义：给定  y \in \mathbf{R},对任意  \Delta y>0,若  P\{y-\Delta y<Y \leq y\}>0,  且对任意  x \in \mathbf{R},  极限 \lim _{\Delta y \rightarrow 0^{\circ}} P\{X \leq x \mid y-\Delta y<Y \leq y\}  存在,称此极限函数为在  Y=y  的条件下,随机变量X的条件分布函数，记作  F_{XY}(x \mid y)</script><h5 id="条件概率密度"><a href="#条件概率密度" class="headerlink" title="条件概率密度"></a>条件概率密度</h5><script type="math/tex; mode=display">\begin{array}{c}F_{X \mid Y}(x \mid y)=\lim _{\Delta y \rightarrow 0^{+}} P\{X \leq x \mid y-\Delta y<Y \leq y\}=\lim _{\Delta y \rightarrow 0^{+}} \frac{P\{X \leq x, y-\Delta y<Y \leq y\}}{P\{y-\Delta y<Y \leq y\}} \\=\lim _{\Delta y \rightarrow 0^{*}} \frac{\int_{y-\Delta y}^{y} \int_{-\infty}^{x} f(u, v) \mathrm{d} u \mathrm{d} v}{\int_{y-\Delta y}^{y} f_{Y}(v) \mathrm{d} v} \approx \frac{\int_{-\infty}^{x} f(u, y) \mathrm{d} u \times \Delta y}{f_{Y}(y) \times \Delta y}=\frac{\int_{-\infty}^{x} f(u, y) \mathrm{d} u}{f_{Y}(y)} \\F_{x \mid Y}(x \mid y)=\int_{-\infty}^{x} \frac{f(u, y)}{f_{Y}(y)} \mathrm{d} u\end{array}</script><script type="math/tex; mode=display">f_{X \mid Y}(x \mid y)=F_{X \mid Y}^{\prime}(x \mid y)=\frac{f(x, y)}{f_{Y}(y)}</script><p>为在 Y=y的条件下随机变量X 的条件概率密度。其中x是自变量，y是固定值。</p><h6 id="如何判断X-Y两个连续型随机变量相互独立"><a href="#如何判断X-Y两个连续型随机变量相互独立" class="headerlink" title="如何判断X,Y两个连续型随机变量相互独立"></a>如何判断X,Y两个连续型随机变量相互独立</h6><script type="math/tex; mode=display">1)  F(x, y)=F_{X}(x) F_{Y}(y) \quad(x, y) \in R^{2} \\2)  f(x, y)=f_{X}(x) f_{Y}(y)  在平面上除夫“面积”为0 的集合外成立。  X  与  Y  相互独立 \\ \Leftrightarrow F_{X \mid Y}(x \mid y)=F(x),  对所有  (x, y) \in R^{2}  成立.\\3)  f_{X}(x)=f_{X \mid Y}(x \mid y)  在平面上除去“面积”为0 的集合外成立。\\4)  f_{Y}(y)=f_{Y X}(y \mid x)</script><h5 id="联合分布，边缘分布，条件分布的关系"><a href="#联合分布，边缘分布，条件分布的关系" class="headerlink" title="联合分布，边缘分布，条件分布的关系"></a>联合分布，边缘分布，条件分布的关系</h5><p>通过联合分布可以求出边缘分布，进一步求得条件分布。然后通过边缘分布和条件分布可以确定唯一一个联合分布。</p><h5 id="离散型随机变量的函数及其分布率"><a href="#离散型随机变量的函数及其分布率" class="headerlink" title="离散型随机变量的函数及其分布率"></a>离散型随机变量的函数及其分布率</h5><h6 id="一维情况"><a href="#一维情况" class="headerlink" title="一维情况"></a>一维情况</h6><p>根据随机变量X的分布律和函数，求出对应变量的值，然后合并函数值相同的取值点对应的概率。</p><h6 id="二维情况"><a href="#二维情况" class="headerlink" title="二维情况"></a>二维情况</h6><p>根据二维离散型随机变量的联合分布率和二元函数求出函数值，然后合并函数值相同的取值点对应的概率。</p><h5 id="离散卷积公式"><a href="#离散卷积公式" class="headerlink" title="离散卷积公式"></a>离散卷积公式</h5><p>设随机变量(X,Y)是离散型随机变量,X,Y相互独立，其分布律为</p><script type="math/tex; mode=display">\begin{array}{l}\boldsymbol{P}\{\boldsymbol{X}=\boldsymbol{k}\}=\boldsymbol{p}_{\boldsymbol{k}} \quad \boldsymbol{k}=\boldsymbol{0}, \mathbf{1}, \boldsymbol{2}, \ldots \\\boldsymbol{P}\{\boldsymbol{Y}=\boldsymbol{r}\}=\boldsymbol{q}_{\boldsymbol{r}} \quad \boldsymbol{r}=\boldsymbol{0}, \boldsymbol{1}, \boldsymbol{2}, \ldots\end{array}</script><p>则X+Y的分布律为</p><script type="math/tex; mode=display">\boldsymbol{P}\{\boldsymbol{X}+\boldsymbol{Y}=\boldsymbol{m}\}=\sum_{k=0}^{m} \boldsymbol{p}_{k} \boldsymbol{q}_{m-k} \quad \boldsymbol{m}=\boldsymbol{0}, \boldsymbol{1}, \boldsymbol{2}, \ldots</script><p>随机变量若相互独立且具有相同类型的分布，则可以相加。二项分布随机变量可等价表示为多个独立0-1分布随机变量之和。</p><h5 id="连续型随机变量的函数及其概率密度"><a href="#连续型随机变量的函数及其概率密度" class="headerlink" title="连续型随机变量的函数及其概率密度"></a>连续型随机变量的函数及其概率密度</h5><h6 id="一维连续型随机变量函数的概率密度"><a href="#一维连续型随机变量函数的概率密度" class="headerlink" title="一维连续型随机变量函数的概率密度"></a>一维连续型随机变量函数的概率密度</h6><script type="math/tex; mode=display">\begin{aligned}F_{Y}(y) &=P\{Y \leq y\}=P\{g(X) \leq y\} \\&=\int\{x \mid g(x)<y\}^{f_{X}}(x) d x\end{aligned}</script><p>然后对F(y)求导即可。解题关键是将g(X)&lt;=y转换为关于X的取值范围并求概率。</p><h5 id="二维连续型随机变量函数的概率密度"><a href="#二维连续型随机变量函数的概率密度" class="headerlink" title="二维连续型随机变量函数的概率密度"></a>二维连续型随机变量函数的概率密度</h5><script type="math/tex; mode=display">\begin{aligned}F_{Z}(z) &=P\{Z \leq z\}=P\{G(X, Y) \leq z\} \\&=\iint_{\{(x, y): G(x, y) \leq z\}} f(x, y) d x d y\end{aligned}</script><p>然后求导得出其概率密度。关键把z看成常数，确定积分上下限。</p><p>Z=X+Y，若随机变量X,Y相互独立，则</p><script type="math/tex; mode=display">\begin{array}{l}f_{Z}(z)=\int_{-\infty}^{+\infty} f_{X}(z-y) f_{Y}(y) d y \\f_{Z}(z)=\int_{-\infty}^{+\infty} f_{X}(x) f_{Y}(z-x) d x\end{array}</script><p>正态分布具有可加性，均匀分布不具有可加性。</p><h5 id="数学期望"><a href="#数学期望" class="headerlink" title="数学期望"></a>数学期望</h5><p>离散型随机变量的期望</p><script type="math/tex; mode=display">E(X)=\sum_{i=1}^{+\infty} x_{i} p_{i}</script><p>连续型随机变量的期望</p><script type="math/tex; mode=display">E(X)=\int_{-\infty}^{+\infty} x f(x) d x</script><p>随机变量的数学期望是它所有可能取值的加权平均值，是一个数字。定义中的绝对收敛是保证数学期望的唯一性。</p><h5 id="随机变量的函数的数学期望"><a href="#随机变量的函数的数学期望" class="headerlink" title="随机变量的函数的数学期望"></a>随机变量的函数的数学期望</h5><p>离散型随机变量</p><script type="math/tex; mode=display">E(Y)=E[g(X)]=\sum_{i=1}^{+\infty} g\left(x_{i}\right) p_{i}</script><p>连续型随机变量</p><script type="math/tex; mode=display">E(Y)=E[g(X)]=\int_{-\infty}^{+\infty} g(x) f_{X}(x) d x</script><h5 id="期望性质"><a href="#期望性质" class="headerlink" title="期望性质"></a>期望性质</h5><p>1.E(c)=c<br>2.E(cX)=cE(X)<br>3.</p><script type="math/tex; mode=display">E\left(\sum_{i=1}^{n} X_{i}\right)=\sum_{i=1}^{n} E\left(X_{i}\right)</script><p>4.如果X,Y相互独立，那么期望的乘积等于乘积的期望<br>5.</p><script type="math/tex; mode=display">若X_{1}, X_{2}, \ldots, X_{n} \text { 相互独立，则 } \quad E\left(\prod_{i=1}^{n} X_{i}\right)=\prod_{i=1}^{n} E\left(X_{i}\right)</script><h5 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h5><script type="math/tex; mode=display">D(X)=E\left(X^{2}\right)-[E(X)]^{2}</script><p>方差刻画了随机变量X相对数学期望的偏离程度，随机变量X关于自身的数学期望的偏离程度比相对其他任何值的偏离程度都小<br>1.常数的方差等于0<br>2.</p><script type="math/tex; mode=display">\boldsymbol{D}(\boldsymbol{c} \boldsymbol{X})=\boldsymbol{c}^{2} \boldsymbol{D}(\boldsymbol{X})</script><p>3.</p><script type="math/tex; mode=display">D\left(\sum_{i=1}^{n} X_{i}\right)=\sum_{i=1}^{n} D\left(X_{i}\right)+2 \sum_{i=1}^{n} E\left\{\left[X_{i}-E\left(X_{i}\right)\right]\left[X_{j}-E\left(X_{j}\right)\right]\right\}</script><p>4.</p><script type="math/tex; mode=display">D\left(\sum_{i=1}^{n} X_{i}\right)=\sum_{i=1}^{n} D\left(X_{i}\right)</script><p>5.</p><script type="math/tex; mode=display">D(X)=0 \quad \longleftrightarrow \quad P\{X=E(X)\}=1</script><p>6.切比雪夫不等式，若随机变量X的方差D(X)存在，则任意</p><script type="math/tex; mode=display">\varepsilon>0</script><p>有</p><script type="math/tex; mode=display">P\{|X-E(X)| \geq \varepsilon\} \leq \frac{D(X)}{\varepsilon^{2}}</script><h5 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h5><p>若E{[X-E(X)][Y-E(Y)]}存在，称</p><script type="math/tex; mode=display">\operatorname{Cov}(X, Y)=E\{[X-E(X)][Y-E(Y)]\}</script><p>为随机变量( X, Y)的协方差.有</p><script type="math/tex; mode=display">\boldsymbol{D}(\boldsymbol{X})=\operatorname{Cov}(\boldsymbol{X}, \boldsymbol{X})</script><script type="math/tex; mode=display">\boldsymbol{D}(\boldsymbol{X} \pm \boldsymbol{Y})=\boldsymbol{D}(\boldsymbol{X})+\boldsymbol{D}(\boldsymbol{Y}) \pm 2 \operatorname{Cov}(\boldsymbol{X}, \boldsymbol{Y})</script><script type="math/tex; mode=display">\operatorname{Cov}\left(X_{1}+X_{2}, Y\right)=\operatorname{Cov}\left(X_{1}, Y\right)+\operatorname{Cov}\left(X_{2}, Y\right)</script><script type="math/tex; mode=display">\operatorname{cov}(X, Y)=E(X Y)-E(X) E(Y)</script><h5 id="相关系数"><a href="#相关系数" class="headerlink" title="相关系数"></a>相关系数</h5><script type="math/tex; mode=display">\rho_{X Y}=\frac{\operatorname{Cov}(X, Y)}{\sqrt{D(X)} \sqrt{D(Y)}}</script><script type="math/tex; mode=display">\begin{aligned}\rho_{X Y} &=E\left[\frac{X-E(X)}{\sqrt{D(X)}} \cdot \frac{Y-E(Y)}{\sqrt{D(Y)}}\right] \\&=E\left[X^{*} Y^{*}\right]=\operatorname{Cov}\left(X^{*}, Y^{*}\right)\end{aligned}</script><h5 id="协方差矩阵"><a href="#协方差矩阵" class="headerlink" title="协方差矩阵"></a>协方差矩阵</h5><p>设n维随机变量(X1,X2,…,Xn)的协方差均存在那么随机变量的协方差矩阵为</p><script type="math/tex; mode=display">C=\left[\begin{array}{cccc}c_{11} & c_{12} & \ldots & c_{1 n} \\c_{21} & c_{22} & \ldots & c_{2 n} \\\cdot & \cdot & \ldots & \cdot \\c_{n 1} & c_{n 2} & \ldots & c_{n n}\end{array}\right]</script><p>性质</p><p>1.Cij=D(Xi)，i=1，2，3…，n<br>2.Cij=Cji<br>3.C是非负定矩阵<br>4.</p><script type="math/tex; mode=display">c_{i j}^{2} \leq c_{i i} \cdot c_{j j}, \quad i, j=1,2, \ldots, n</script><h5 id="矩"><a href="#矩" class="headerlink" title="矩"></a>矩</h5><p>设随机变量X，</p><script type="math/tex; mode=display">\gamma_{k}=E\left(X^{k}\right) \quad k=1,2,3 \ldots</script><p>为X的k阶原点矩</p><script type="math/tex; mode=display">\alpha_{k}=E\left(|X|^{k}\right), \quad k=1,2,3 \ldots \ldots</script><p>为X的k阶绝对原点矩。</p><script type="math/tex; mode=display">\mu_{k}=E\left\{[X-E(X)]^{k}\right\} k=1,2,3 \ldots \ldots</script><p>为X的k阶中心矩</p><h5 id="依概率收敛"><a href="#依概率收敛" class="headerlink" title="依概率收敛"></a>依概率收敛</h5><p>设{Xn}是一个随机变量序列,X是一个随机变量或常数，若对任意<br><pre><code class="hljs math">\varepsilon &gt;0</code></pre><br>有，</p><script type="math/tex; mode=display">\begin{aligned}&\lim _{n \rightarrow \infty} P\left\{\left|X_{n}-X\right| \geq \varepsilon\right\}=0\\&\text { 或 } \lim _{n \rightarrow \infty} P\left\{\left|X_{n}-X\right|<\varepsilon\right\}=1\end{aligned}</script><p>则称随机变量序列{Xn}依概率收敛于X。表示当n很大时，Xn与X出现较大偏差的可能性很小。</p><h5 id="大数定律"><a href="#大数定律" class="headerlink" title="大数定律"></a>大数定律</h5><p>设Xn，n=1，2…是随机变量序列，其数学期望都存在，若对于任意的</p><script type="math/tex; mode=display">\varepsilon >0</script><p>有</p><script type="math/tex; mode=display">\lim _{n \rightarrow \infty} P\left\{\left|\frac{1}{n} \sum_{i=1}^{n} X_{i}-\frac{1}{n} \sum_{i=1}^{n} E\left(X_{i}\right)\right|<\varepsilon\right\}=1</script><p>称随机变量序列{Xn}服从大数定律。<br>大数定律的概率意义：{Xk}的前n项算术平均将紧密地聚集在其数学期望的附近。</p><h5 id="切比雪夫大数定律"><a href="#切比雪夫大数定律" class="headerlink" title="切比雪夫大数定律"></a>切比雪夫大数定律</h5><p>设Xk是相互独立的随机变量序列，其数学期望和方差都存在，且存在常数C，使得</p><script type="math/tex; mode=display">D\left(X_{k}\right)<C, k=1,2, \ldots</script><p>则随机变量序列{Xk}服从大数定律。</p><h5 id="切比雪夫不等式"><a href="#切比雪夫不等式" class="headerlink" title="切比雪夫不等式"></a>切比雪夫不等式</h5><p>设随机变量X的数学期望E(X)和方差D(X)都存在，则对于任意的</p><script type="math/tex; mode=display">\varepsilon >0</script><p>有</p><script type="math/tex; mode=display">\begin{aligned}& P\{|X-E(X)| \geq \varepsilon\} \leq \frac{D(X)}{\varepsilon^{2}} \\\text { 或者 } & P\{|X-E(X)|<\varepsilon\} \geq 1-\frac{D(X)}{\varepsilon^{2}}\end{aligned}</script><p>该公式刻画的是其随机变量与数学期望的概率关系，是对方差存在的随机变量落在以数学期望为中心的对称区间内的概率的粗略估计。其方差越小，说明了随机变量和数学期望距离小的概率越大。</p><h5 id="独立同分布大数定律"><a href="#独立同分布大数定律" class="headerlink" title="独立同分布大数定律"></a>独立同分布大数定律</h5><p>设X1,X2…Xn…是相互独立的随机变量，且E(xi)=μ，D(Xi)=σ^2,i=1,2…则</p><script type="math/tex; mode=display">\lim _{n \rightarrow \infty} P\left\{\left|\frac{1}{n} \sum_{i=1}^{n} X_{i}-\mu\right|<\varepsilon\right\}=1(\varepsilon>0)</script><h5 id="伯努利大数定律"><a href="#伯努利大数定律" class="headerlink" title="伯努利大数定律"></a>伯努利大数定律</h5><p>设m为n重伯努利试验中事件A出现的次数，p为A在每次实验中发生的概率，则</p><script type="math/tex; mode=display">\lim _{n \rightarrow \infty} P\left\{\left|\frac{m}{n}-p\right|<\varepsilon\right\}=1(\varepsilon>0)</script><h5 id="依分布收敛"><a href="#依分布收敛" class="headerlink" title="依分布收敛"></a>依分布收敛</h5><p>对于同一分布的随机变量，随着叠加次数的增大，逐渐趋向于正态分布。设X1,X2,…Xn,…是随机变量序列，X为随机变量，Fn(x)和F(x)分别是Xn和X的分布函数，如果在F(x)的连续点处均有</p><script type="math/tex; mode=display">\lim _{n \rightarrow \infty} F_{n}(x)=F(x)</script><p>则称随机变量序列X1,X2,…Xn,…依分布收敛于X.依分布收敛但不一定满足依概率收敛。</p><h5 id="中心极限定理"><a href="#中心极限定理" class="headerlink" title="中心极限定理"></a>中心极限定理</h5><p>设{Xk},k=1,2…为相互独立，具有相同分布的随机变量序列，且E(Xk)=μ，D(Xk)=σ^2,(k=1,2…),则{Xk}服从中心极限定理，即</p><script type="math/tex; mode=display">\begin{array}{c}\lim _{n \rightarrow \infty}\left\{\frac{\sum_{k = 1}^{n} X_{k}-E\left(\sum_{k = 1}^{n} X_{k}\right)}{\sqrt{D\left(\sum_{k = 1}^{n} X_{k}\right)}}\} = \Phi(x), x \in R\right.\end{array}</script><p>{Xk}服从中心极限定理的含义：{Xk}前n项和的标准化随机变量序列依分布收敛于标准正态分布随机变量。夺格均匀地小的独立随机变量的叠加，其分布近似正态分布。</p><h3 id="数理统计的基本概念"><a href="#数理统计的基本概念" class="headerlink" title="数理统计的基本概念"></a>数理统计的基本概念</h3><h5 id="总体，样本的概念"><a href="#总体，样本的概念" class="headerlink" title="总体，样本的概念"></a>总体，样本的概念</h5><p>总体指研究对象的全体。个体指组成总体的每个元素。</p><h5 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h5><p>思想方法：一个随机试验有若干可能结果，某结果发生了，则认为该结果发生的概率最大，n维随机变量的联合概率密度称为似然函数。极大似然估计法是求参数θ的估计值，使似然函数达到极大值。极大似然估计值仍是一个函数，极大似然估计量是一个数。</p><h5 id="矩估计法。"><a href="#矩估计法。" class="headerlink" title="矩估计法。"></a>矩估计法。</h5><p>先计算一阶原点矩即数学期望。然后让样本矩估计等于总体矩估计，就可求得参数。矩估计量与极大似然估计量不一定完全相同。用矩估计来估计参数比较方便，但数据量较大时极大似然估计法更加准确。</p><h5 id="估计量的优良性准则"><a href="#估计量的优良性准则" class="headerlink" title="估计量的优良性准则"></a>估计量的优良性准则</h5><ol><li>无偏估计，设θ^是未知参数θ的估计量，若E(θ^)=0,则称θ^为θ的无偏估计。<br>2.有效性，比较两个总体参数估计量的方差，哪一个方差小则更有效。<br>3.相合性，若估计参数依概率收敛于真实参数的值，则称改估计参数为未知参数的相和估计量。只要样本n足够大，用θ^去估计θ其误差可以任意小。<h5 id="区间估计"><a href="#区间估计" class="headerlink" title="区间估计"></a>区间估计</h5>设总体的未知参数为θ，由样本确定两个统计量。对于给定实数满足</li></ol><script type="math/tex; mode=display">\boldsymbol{P}\left\{\hat{\theta}_{1}\left(\boldsymbol{X}_{1}, \ldots, \boldsymbol{X}_{n}\right) \leq \theta \leq \hat{\theta}_{2}\left(\boldsymbol{X}_{1}, \ldots, \boldsymbol{X}_{n}\right)\right\}=\mathbf{1}-\alpha</script><p>随机区间[θ^1,θ^2]为θ的置信度为1-α的置信区间。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>概率论（上）</title>
    <link href="/2020/09/23/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    <url>/2020/09/23/%E6%A6%82%E7%8E%87%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h3 id="一些基本概念"><a href="#一些基本概念" class="headerlink" title="一些基本概念"></a>一些基本概念</h3><ol><li>确定性现象特点 ： 可事前预言或描述</li><li>非确定性现象特点 ： 事前不可预言，但可能遵循某种规律</li><li>随机现象 ： 在个别实验中结果出现不确定性，但再大量重复实验中又呈现出规律性</li><li>随机实验三个特点 ： 重复性；结果明确性；不可预知性</li><li>随机试验中可能发生也可能不发生的事情称为随机事件。</li><li>必然事件 ： 随机试验中肯定发生的事情。</li><li>不可能事件 ： 随机试验中肯定不发生的事件。</li><li>基本实验 ： 在一次试验中必发生一个且仅发生一个的最简单事件。</li><li>复合事件 ： 由若干基本事件组合而成的事件</li><li>样本空间  随机事件中的基本事件对应样本空间的单点子集，那么随机事件都可以通过样本空间一一映射到集合中。</li></ol><p>另外实验目的不同可能导致基本事件及样本空间不同</p><h3 id="事件之间的关系及其基本运算"><a href="#事件之间的关系及其基本运算" class="headerlink" title="事件之间的关系及其基本运算"></a>事件之间的关系及其基本运算</h3><h5 id="随机事件的关系及其运算实质上对应集合的关系及运算。"><a href="#随机事件的关系及其运算实质上对应集合的关系及运算。" class="headerlink" title="随机事件的关系及其运算实质上对应集合的关系及运算。"></a>随机事件的关系及其运算实质上对应集合的关系及运算。</h5><h5 id="包含关系"><a href="#包含关系" class="headerlink" title="包含关系"></a>包含关系</h5><p>若事件A发生，必然导致事件B发生，则称事件B包含事件A，或称A是B的子事件。<br>对于任意事件有，不可能事件包含于A包含于样本空间<br>如果两个事件相互包含则称两事件相等</p><h5 id="和事件"><a href="#和事件" class="headerlink" title="和事件"></a>和事件</h5><p>事件A和B至少有一个发生称为事件A与B的和事件，就是两个集合的并集。</p><h5 id="积事件"><a href="#积事件" class="headerlink" title="积事件"></a>积事件</h5><p>A与B同时发生称为事件A和B的积事件，记作AB。就是A和B的交集。</p><h5 id="互不相容"><a href="#互不相容" class="headerlink" title="互不相容"></a>互不相容</h5><p>如果AB=不可能事件，称A,B互不相容或互斥事件。同一事件所有基本事件互不相容，不可能事件与所有事件互不相容</p><p>==推广：做一次试验，事件组中任意两个互不相容，则称此事件组互不相容。==<br>事件组互不相容指的是任意有限个互不相容。</p><h5 id="（5）逆事件（对立事件）"><a href="#（5）逆事件（对立事件）" class="headerlink" title="（5）逆事件（对立事件）"></a>（5）逆事件（对立事件）</h5><p>如果AB=不可能事件，A并B为样本空间，称A,B为对立事件。</p><h5 id="差事件"><a href="#差事件" class="headerlink" title="差事件"></a>差事件</h5><p>事件A发生并且B不发生，称为事件A与B的差事件。记为A-B</p><h5 id="随机事件运算律"><a href="#随机事件运算律" class="headerlink" title="随机事件运算律"></a>随机事件运算律</h5><ul><li>交换律</li><li>结合律</li><li>分配律</li><li>德.摩根律</li><li>吸收率</li></ul><h3 id="概率"><a href="#概率" class="headerlink" title="概率"></a>概率</h3><h5 id="概率-1"><a href="#概率-1" class="headerlink" title="概率"></a>概率</h5><p>概率是刻画随机事件发生可能性大小的数量指标。它不依主观变化而变化。事件A概率为P(A)，且0&lt;=P(A)&lt;=1。</p><h5 id="频率"><a href="#频率" class="headerlink" title="频率"></a>频率</h5><p>在先沟通条件下，进行了n次实验，事件A发生了m次，称比值</p><script type="math/tex; mode=display">f_n(A)=\frac{n}{m}</script><ul><li>频率从一定程度上反映了事件发生可能性的大小，它随着试验的次数、试验者的变化会有所不用。</li><li>频率具有稳定性，在一定条件下，频率稳定于某个常数。</li><li>频率不确定性，不会随着试验次数增大趋于某个特定常数。<h5 id="古典概型"><a href="#古典概型" class="headerlink" title="古典概型"></a>古典概型</h5>如果满足==仅有有限多个基本事件，每个基本事件发生的可能性相等==才是古典概型。才可以用样本点次数/样本空间总点数来计算概率。<h5 id="概率的公理化定义"><a href="#概率的公理化定义" class="headerlink" title="概率的公理化定义"></a>概率的公理化定义</h5></li><li>概率是随机事件发生可能性大小的客观度量。</li><li>频率不是概率。频率具有不可预言性</li><li>古典概率不能广泛使用于各类随机试验。古典概型试验具有有限多个基本事件和每个基本事件发生的可能性相等。所以有局限性。</li><li>几何测度和几何概率。集合概率是为了突破古典概型的局限性。对物体的量化描述，如长度，面积等成为几何量度。几何概率定义：设样本空间可用欧氏空间的子集s表示，而且s及其全体子集A均可用几何测度度量，称度量值之比为事件A发生的几何概率。不过由于集合概率要求样本点在样本空间分布具有均匀性，所以几何概率也有明显局限性。</li><li>对于随机试验E的样本空间为Ω，若对于E的每一个事件都赋予一个实数P(A)，则对应规则满足非负性，规范性（P(Ω)=1），==可列可加性（互不相容事件和事件概率为各互不相容事件概率之和）。==<h5 id="概率的基本性质"><a href="#概率的基本性质" class="headerlink" title="概率的基本性质"></a>概率的基本性质</h5></li><li>不可能事件的概率为0</li><li>概率具有有限可加性</li><li>对任意事件，该事件与其对立事件之和等于1 </li><li>单调性，若随机事件A和B满足A含于B，则P(A)&lt;=P(B),P(B-A)=P(B)-P(A)</li><li>概率加法定理，对于任意两个随机事件A和B有P(A∪B)=P(A)+P(B)-P(AB)<h5 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h5>已知事件B发生条件下，事件A发生的客观度量称为条件概率。我们用记号P(A|B)来表示条件概率。P(A|B)=P(AB)/P(B),既然是概率所以满足概率的三条性质。</li></ul><p>条件概率P(A|B)与无条件概率P(A)之间没有确定的大小关系。对于条件概率P(A|B),Ω∩B上的条件概率。</p><h6 id="概率乘法公式"><a href="#概率乘法公式" class="headerlink" title="概率乘法公式"></a>概率乘法公式</h6><p>设P(B)&gt;0，则有P(AB)=P(B)P(A|B)，另有P(A|ABC)=P(A)P(B|A)P(C|AB)。乘法公式是计算事件积的概率的公式。==使用关键是确定用什么样的事件做条件。==先发生的事件或者作为原因的事件适合做条件。</p><h5 id="全概率公式"><a href="#全概率公式" class="headerlink" title="全概率公式"></a>全概率公式</h5><p>定义： 设Ω为随机试验的样本空间，B1,B2,B3…Bn为一组事件，若</p><ul><li>B1，B2……Bn各事件互斥</li><li>B1∪B2∪Bn=Ω即对Ω的有限划分<br>则有<script type="math/tex; mode=display">P(A)=\sum_{i=1}^{n} p\left(B_{i}\right)P(A|B_i)</script></li></ul><p>全概率公式体现了一种概率的分解计算思想，应用关键：正确寻找样本空间的有限划分。全概率公式常用于对各原因导致一个结果发生的知因求果型问题的概率预测。</p><h5 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h5><p>把已发生的结果记为事件A，可能引起A发生的所有相异原因记为事件B1，B2,……，Bn。B为Ω的有限划分。就是关注A已发生条件下，某原因Bi导致其发生的概率。</p><script type="math/tex; mode=display">\begin{aligned}P(B, \mid A) &=\frac{P\left(A B_{j}\right)}{P(A)} \\&=\frac{P\left(B_{j}\right) P\left(A \mid B_{j}\right)}{P(A)} \\&=\frac{P\left(B_{j}\right) P\left(A \mid B_{j}\right)}{\sum_{i=1}^{n} P\left(B_{i}\right) P\left(A \mid B_{i}\right)}\end{aligned}</script><h5 id="事件的独立性"><a href="#事件的独立性" class="headerlink" title="事件的独立性"></a>事件的独立性</h5><p>设A,B是试验E的两个事件，若满足P(A|B)=P(A)或P(B|A)=P(B)或P(AB)=P(A)P(B)，则称事件A与B相互独立。</p><ul><li>定理1 若事件A与B相互独立，则A与B的对立事件，B与A的对立事件，B的对立事件与A的对立事件也相互独立。 </li><li>两两独立就是任意n个事件之间的两个事件相互独立。</li><li>若事件列An相互独立，则将该事件列中任意多个事件换成它们的对立事件后，所得到的n个事件依然相互独立。<h3 id="随机变量的分布"><a href="#随机变量的分布" class="headerlink" title="随机变量的分布"></a>随机变量的分布</h3>设E的样本空间为Ω，对于每一个样本点w∈Ω，都有唯一实数X(W)与之对应，且对于任意实数x，事件{W|X(W)&lt;=x}都有确定的概率，则称X（w）为随机变量，简记为X。<h5 id="分布函数"><a href="#分布函数" class="headerlink" title="分布函数"></a>分布函数</h5>设X是样本空间Ω上的随机变量，x是任意实数，称函数<script type="math/tex; mode=display">F(x)=P\{X \leq x\}=P\{\omega \mid X(\omega) \leq x\}</script>为随机变量X的分布函数。<h5 id="分布函数性质"><a href="#分布函数性质" class="headerlink" title="分布函数性质"></a>分布函数性质</h5></li></ul><ol><li>F(X)单调不减</li><li>0&lt;=F(X)&lt;=1且<script type="math/tex; mode=display">\lim _{x \rightarrow-\infty} F(x)=0, \lim _{x \rightarrow \infty} F(x)=1</script></li></ol><p>3.F(X)右连续，即F(X+0)=F(X)<br>注：P(X=x)=F(x)-F(x-0)<br>如果某个函数满足上面三个式子，那一定是分布函数。</p><h5 id="离散型随机变量"><a href="#离散型随机变量" class="headerlink" title="离散型随机变量"></a>离散型随机变量</h5><p>随机变量分为离散型随机变量与 非离散型随机变量两种，随机变量的函数仍为随机变量。有些随机变量,它全部可能取到的不相同的值是有限个或可列无限多个，也可以说概率1以一定的规律分布在各个可能值上。这种随机变量称为”离散型随机变量”。</p><p>离散型随机变量的概率分布有两条基本性质：</p><ul><li>(1)Pn≥0 n=1,2,…</li><li>(2)∑pn=1<h5 id="二项分布"><a href="#二项分布" class="headerlink" title="二项分布"></a>二项分布</h5>一般地，如果随机变量服从参数为和的二项分布，我们记为或。n次试验中正好得到k次成功的概率由概率质量函数给出：</li></ul><script type="math/tex; mode=display">P\{X=k\}=C_k^n p^{k}(1-p)^{n-k}</script><p>式中k=0，1，2，…，n,是二项式系数（这就是二项分布名称的由来）。</p><h5 id="泊松分布"><a href="#泊松分布" class="headerlink" title="泊松分布"></a>泊松分布</h5><p>泊松分布的概率函数为:</p><script type="math/tex; mode=display">P(X=k)=\frac{\lambda^{k}}{k !} e^{-\lambda}, k=0,1, \cdots</script><p>泊松分布的参数入是单位时间(或单位醒积)内随机事件的平均发生次数。泊松分布适合于描述单位时间内随机事件发生的<br>次数。<br>泊松分布的期望和方差均为 λ</p><p>特征函数为  </p><script type="math/tex; mode=display">\psi(t)=\exp \left\{\lambda\left(e^{i t}-1\right)\right\}</script><p>泊松分布可看成二项分布的极限分布。在大量次独立重复中“稀有事件”出现次数可认为服从泊松分布。</p><h4 id="连续型随机变量"><a href="#连续型随机变量" class="headerlink" title="连续型随机变量"></a>连续型随机变量</h4><p>设随机变量X的分布函数为F(X)，若存在非负函数f(x),对于任意实数x，均有</p><script type="math/tex; mode=display">F(x)=\int_{-\infty}^{x} f(t) d t</script><p>则称随机变量X是连续型随机变量。称f(x)为X的概率密度函数，简称概率密度。密度曲线的高低反映了随机变量在各处取值概率的大小。</p><ul><li>连续型随机变量X的分布函数是连续函数。</li><li>X是连续型随机变量，则对任意实数x0属于R，有P{X=x0}=0。</li><li>P{Φ}=0<h5 id="均匀分布"><a href="#均匀分布" class="headerlink" title="均匀分布"></a>均匀分布</h5>定义：设随机变量X 的概率密度函数为</li></ul><script type="math/tex; mode=display">f(x)=\left\{\begin{array}{ll}\frac{1}{b-a}, & x \in(a, b) \\0, & x \notin(a, b)\end{array}\right.</script><p>称随机变量X在区间 </p><script type="math/tex; mode=display">(\boldsymbol{a}, \boldsymbol{b})</script><p> 上服从<br>均匀分布。记为</p><script type="math/tex; mode=display">X  \sim U(a, b)</script><p> 特点：如果随机变量X落在(a,b)的子区间的概率与位置无关，与子区间的长度成正比。</p><p>== 服从均匀分布的随机变量落在长度相同的区间内的概率相等。==</p><h5 id="指数分布"><a href="#指数分布" class="headerlink" title="指数分布"></a>指数分布</h5><p>定义</p><script type="math/tex; mode=display">f(x)=\left\{\begin{array}{ll}\lambda e^{-\lambda x}, & x>0 \\0, & x \leq 0\end{array} \lambda>0\right.</script><p>则称随机变量X服从参数为λ的指数分布. </p><script type="math/tex; mode=display">X\sim E(\lambda)</script><p>重要性质：无记忆性</p><script type="math/tex; mode=display">\begin{aligned}&\text { 即: } \quad P\{X>t+s \mid X>t\}=P\{X>s\}\\&\begin{aligned}P\{X>s+t \mid X>t\} &=\frac{P\{X>s+t, X>t\}}{P\{X>t\}} \\&=\frac{P\{X>s+t\}}{P\{X>t\}}=\frac{1-F(s+t)}{1-F(t)} \\&=\frac{e^{-\lambda(s+t)}}{e^{-\lambda t}}=e^{-\lambda s}=P\{X>s\}\end{aligned}\end{aligned}</script><div class="table-container"><table><thead><tr><th>泊松分布</th><th>指数分布</th></tr></thead><tbody><tr><td>特点：某段时间t内，事件次数的概率</td><td>特点：事件的时间间隔的概率</td></tr></tbody></table></div><script type="math/tex; mode=display">泊松分布的分布函数\\\begin{array}{l}P(N(t)=n)=\frac{(\lambda t)^{n} e^{-\lambda t}}{n !}  \\N(t) \sim P(\lambda t) \quad=\frac{(\lambda t)^{0} e^{-\lambda t}}{0 !}\end{array}</script><p>那么指数分布分布函数就是泊松分布随机变量等于0的情况，即</p><script type="math/tex; mode=display">\{X>t\}=\{N(t)=0\}</script><h5 id="正态分布的概率分布"><a href="#正态分布的概率分布" class="headerlink" title="正态分布的概率分布"></a>正态分布的概率分布</h5><p>设随机变量  X  的概率密度函数为</p><script type="math/tex; mode=display">\varphi\left(x ; \mu, \sigma^{2}\right)=\frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}, x \in R\\ \quad X \sim N\left(\mu, \sigma^{2}\right)</script><p>其中  </p><script type="math/tex; mode=display">\mu, \sigma(\sigma>\mathbf{0})</script><p> 是常数，  则称随机变量  X  服从参数为 </p><script type="math/tex; mode=display">\mu, \sigma^{2}</script><p> 的正态分布 (或高斯分布)。</p><script type="math/tex; mode=display">当\mu=\mathbf{0}, \sigma=\mathbf{1}  时，其概率密度函数为\varphi(x)=\frac{1}{\sqrt{2 \pi}} e^{-\frac{x^{2}}{2}}, x \in R\\则称随机变量X服从标准正态分布</script><p>特征：</p><ul><li>概率曲线下总面积为1</li><li>曲线关于x=μ对称，并且P{μ-x&lt;X&lt;μ}=P{μ&lt;X&lt;μ+x}</li><li>曲线在x=μ处取得最大值<script type="math/tex; mode=display">\frac{1}{\sqrt{2 \pi} \sigma}</script></li></ul><p>若随机变量 </p><script type="math/tex; mode=display">X \sim N\left(\mu, \sigma^{2}\right), \quad</script><p> 其分布函数为</p><script type="math/tex; mode=display">{\Phi\left(x ; \mu, \sigma^{2}\right)=\int_{-\infty}^{x} \varphi\left(t ; \mu, \sigma^{2}\right) d t}\\x \in R\\{\Phi\left(x ; \mu, \sigma^{2}\right)=\Phi\left(\frac{x-\mu}{\sigma}\right)}</script><p>正态分布变量取值具有对称性，且概率分布具有中间大，两头小的特征。因此人的身高，考试成绩，测量误差，热噪声均可看成正态分布。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
